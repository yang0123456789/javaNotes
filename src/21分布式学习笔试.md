21分布式学习笔试

# 1.分布式架构概况

https://www.zhihu.com/question/23645117/answer/124708083

\1. **分布式存储系统**
\2. **分布式计算系统**
\3. **分布式管理系统**

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/1089769-20171110170521606-1076859931.png)

- 负载均衡：

　　　　Nginx：高性能、高并发的web服务器；功能包括负载均衡、反向代理、静态内容缓存、访问控制；工作在应用层

　　　　LVS： Linux virtual server，基于集群技术和Linux操作系统实现一个高性能、高可用的服务器；工作在网络层

- webserver：

　　　　Java：Tomcat，Apache，Jboss

　　　　Python：gunicorn、uwsgi、twisted、webpy、tornado

- service：　　

　　　　SOA、微服务、spring boot，django

- 容器：

　　　　docker，kubernetes

- cache：

　　　　memcache、redis等

- 协调中心：

　　　　zookeeper、etcd等

　　　　zookeeper使用了Paxos协议Paxos是强一致性，高可用的去中心化分布式。zookeeper的使用场景非常广泛，之后细讲。

- rpc框架：

　　　　grpc、dubbo、brpc

　　　　dubbo是阿里开源的Java语言开发的高性能RPC框架，在阿里系的诸多架构中，都使用了dubbo + spring boot

- 消息队列：

　　　　kafka、rabbitMQ、rocketMQ、QSP

　　　　消息队列的应用场景：异步处理、应用解耦、流量削锋和消息通讯

- 实时数据平台：

　　　　storm、akka

- 离线数据平台：

　　　　hadoop、spark

　　　　PS: apark、akka、kafka都是scala语言写的，看到这个语言还是很牛逼的

- dbproxy：

　　　　cobar也是阿里开源的，在阿里系中使用也非常广泛，是关系型数据库的sharding + replica 代理

- db：

　　　　mysql、oracle、MongoDB、HBase

- 搜索：

　　　　elasticsearch、solr

- 日志：

　　　　rsyslog、elk、flume



关于分布式系统的知识，可以从大学教科书上找到很多，比如许多人都知道的Andrew S. Tanenbaum等人在2002年出版的《分布式系统原理与范型》（Distributed Systems: Principles and Paradigms）这本书。其实，分布式系统的理论出现于上个世纪70年代，“Symposium on Principles of Distributed Computing（PODC）”和“InternationalSymposium on Distributed Computing （DISC）”这两个分布式领域的学术会议分别创立于1982年和1985年。

# 2.学习路线

由于分布式系统所涉及到的领域众多，知识庞杂，很多新人在最初往往找不到头绪，不知道从何处下手来一步步学习分布式架构。

本文试图通过一个最简单的、常用的分布式系统，来阐述分布式系统中的一些基本问题。

- 负载均衡
- 分布式缓存
- 分布式文件系统/CDN
- 分布式RPC
- 分布式数据库/Nosql
- 分布式消息中间件
- 分布式session问题 
  -总结

下图为一个中大型网站/App的基本架构： 

![这里写图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20160904104101318)

在这个架构中，涉及到以上所列的基本问题：

#负载均衡

负载均衡是分布式系统中的一个最最基本的问题。在上图中：

网关需要把请求分发给不同的Tomcat； 
Tomcat需要把收到的请求，分发给不同的Service；

这都需要负载均衡。一句话：凡是请求从一个入口进来，需要分发给后端不同的机器时，就需要负载均衡。

局域网负载均衡

在上图中，负载均衡发生在局域网内部。在这里，常用的网关软件有Nginx/HAProxy/F5/LVS/各种云上的SLB等。

广域网负载均衡

在上图之外，还有广域网负载均衡。这通常发生在域名服务器上，而不是局域网内部。 
同1个域名，映射到不同的局域网集群。

负载均衡算法

常用的负载均衡算法：随机，轮询（Round Robin)，最小资源数，hash。

#分布式缓存

在上图中，当DB负载过高，我需要为Service机器加缓存时，就遇到一个基本问题： 
如果使用local的内存做缓存，则其他Service机器就没办法共用此缓存。 
因次，我需要一个可以让所有Service机器共享的缓存，这就是分布式缓存。

常用的分布式缓存组件：Memcached/Redis/Tair等

分布式文件系统

在上图中，当我要存储客户端上传的图片文件时，就会遇到另一个基本问题：我不能把图片存在每个Tomcat的本地文件系统里面，这样的话，其他机器就没办法访问了。我需要一个让所有机器可以共享的文件系统，这就是分布式文件系统。

常用的分布式文件系统：MogileFS/TFS/HDFS/Amazon S3/OpenStack Swift等

当使用了分布式文件系统，对外提供图片url访问服务时，就会遇到另一个基本问题：如果每次文件的访问，都要到分布式文件系统里面去取，效率和负载就可能成为问题。 
为此，就需要引入CDN。

常用的CDN厂商，比如ChinCache。当然，现在的各种云存储，比如七牛云，阿里云，腾讯云，已经自带了CDN。

 分布式RPC

分布式系统的一个基本问题就是：机器与机器之间如何通信？ 我们都知道底层原理是TCP/IP，Socket。

但一般很少有人会去裸写Socket，实现机器之间的通信。这里，最常用的组件就是RPC。

最简单的实现RPC的方式就是使用http。当然，业界有很多成熟的开源RPC框架，如Facebook的Thrift, 阿里的Dubbo，点评的Pigeon。。

在RPC内部，一般都自己实现了负载均衡。还有更复杂的，如多版本，服务降级等。

补充一句：虽然底层原理都是Socket，但使用不同框架/组件时，通常都有其自己的跨机器通信方式，比如Mysql JDBC，RPC， 消息中间件等。

分布式数据库

在上图中，DB是单一节点。当访问量达到一定程度，就会涉及到Mysql的分库分表问题。

分库/分表之后，就会涉及到join的问题，分布式事务的问题。

关于分库分表，业界也早有成熟方案。对上层屏蔽分库分表，sql的执行，像是在单库一样。

还有像MongoDB这种Nosql数据库，天生是分布式的。但同样会面对Mysql分库分表所要面对的问题。

还有像阿里的OceanBase，有Mysql的强一致性保证，又是分布式的，还可以支持分布式事务。

分布式消息中间件

在上图中，没有提及到消息中间件。相对其他基本问题，这个需要一个更适合的业务场景来谈，在以后的章节中，会再详述。

常用的消息中间件，比如老一辈的ActiveMQ/RabbitMQ， 新一点的，阿里的RocketMQ，LinkedIn的Kafka等。

消息中间件的一个典型场景就是：通过最终一致性，解决上面的分布式事务问题。

分布式session问题

在传统的单机版应用中，我们经常使用session。而当单机扩展到多机，单机的session就没办法被其他机器所访问。

此时就需要使用分布式session，把session存放在一个所有Tomcat都可以访问的地方。

关于分布式session，业界早有成熟方案，在此不再详述。

总结

本文罗列了分布式系统的各种基本问题和业界常用的技术，希望建立起分布式系统的一个宏观图谱。

后续，会针对各个领域，逐个剖析！



可以从这几个角度去学习

- 负载均衡
- 分布式缓存
- 分布式文件系统/CDN
- 分布式RPC
- 分布式数据库/Nosql
- 分布式消息中间件
- 分布式session问题

具体的可以百度查查，里面很多的。

# 3.Zookeeper入门

Zookeeper 分布式服务框架是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。

简单的说，zookeeper=文件系统+通知机制。

zookeeper功能非常强大，可以实现诸如分布式应用配置管理、统一命名服务、状态同步服务、集群管理等功能

1、 文件系统

Zookeeper维护一个类似文件系统的数据结构：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/201807121434154)



每个子目录项如 NameService 都被称作为 znode(目录节点)，和文件系统一样，我们能够自由的增加、删除znode，在一个znode下增加、删除子znode，唯一的不同在于znode是可以存储数据的。

有四种类型的znode：

- **PERSISTENT-持久化目录节点**

  客户端与zookeeper断开连接后，该节点依旧存在

- **PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点**

  客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号

- **EPHEMERAL-临时目录节点**

  客户端与zookeeper断开连接后，该节点被删除

- **EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点**

  客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号

**2、 监听通知机制**

客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。

## 我们能用zookeeper做什么

### 1、 命名服务

​    这个似乎最简单，在zookeeper的文件系统里创建一个目录，即有唯一的path。在我们使用tborg无法确定上游程序的部署机器时即可与下游程序约定好path，通过path即能互相探索发现，不见不散了。

 

### 2、 配置管理

​    程序总是需要配置的，如果程序分散部署在多台机器上，要逐个改变配置就变得困难。好吧，现在把这些配置全部放到zookeeper上去，保存在 Zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用程序就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中就好。

​                     ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213345_625.png)

 

### 3、 集群管理

所谓集群管理无在乎两点：是否有机器退出和加入、选举master。

​    对于第一点，所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。新机器加入 也是类似，所有机器收到通知：新兄弟目录加入，highcount又有了。

​    对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。

​                 ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213345_947.png)

 

4、 分布式锁

​    有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。

​    对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。

​    对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。

​                     ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213345_5.png)

5、队列管理

两种类型的队列：

1、 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。

2、队列按照 FIFO 方式进行入队和出队操作。

第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。

第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。         

​     终于了解完我们能用zookeeper做什么了，可是作为一个程序员，我们总是想狂热了解zookeeper是如何做到这一点的，单点维护一个文件系统没有什么难度，可是如果是一个集群维护一个文件系统保持数据的一致性就非常困难了。

 

从客户端读写访问的透明度来看，数据复制集群系统分下面两种：

1、写主(WriteMaster)
对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离；

2、写任意(Write Any)
对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。

 

​    对zookeeper来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这 也是它建立observer的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。

我们关注的重点还是在如何保证数据在集群所有机器的一致性，这就涉及到paxos算法。

 

### 数据一致性与paxos算法

​    据说Paxos算法的难理解与算法的知名度一样令人敬仰，所以我们先看如何保持数据的一致性，这里有个原则就是：

在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。

​    Paxos算法解决的什么问题呢，解决的就是保证每个节点执行相同的操作序列。好吧，这还不简单，master维护一个全局写队列，所有写操作都必须 放入这个队列编号，那么无论我们写多少个节点，只要写操作是按编号来的，就能保证一致性。没错，就是这样，可是如果master挂了呢。

​    Paxos算法通过投票来对写操作进行全局编号，同一时刻，只有一个写操作被批准，同时并发的写操作要去争取选票，只有获得过半数选票的写操作才会被 批准（所以永远只会有一个写操作得到批准），其他的写操作竞争失败只好再发起一轮投票，就这样，在日复一日年复一年的投票中，所有写操作都被严格编号排 序。编号严格递增，当一个节点接受了一个编号为100的写操作，之后又接受到编号为99的写操作（因为网络延迟等很多不可预见原因），它马上能意识到自己 数据不一致了，自动停止对外服务并重启同步过程。任何一个节点挂掉都不会影响整个集群的数据一致性（总2n+1台，除非挂掉大于n台）。

总结

​    Zookeeper 作为 Hadoop 项目中的一个子项目，是 Hadoop 集群管理的一个必不可少的模块，它主要用来控制集群中的数据，如它管理 Hadoop 集群中的 NameNode，还有 Hbase 中 Master Election、Server 之间状态同步等。

## Zookeeper的基本概念

### 1.1 角色

Zookeeper中的角色主要有以下三类，如下表所示：

​             ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213346_932.png)

系统模型如图所示：

​             ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213346_540.jpg)    

### 1.2 设计目的

1.最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。

2 .可靠性：具有简单、健壮、良好的性能，如果消息m被到一台服务器接受，那么它将被所有的服务器接受。

3 .实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。

4 .等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。

5.原子性：更新只能成功或者失败，没有中间状态。

6 .顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。

##  ZooKeeper的工作原理

​    Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分 别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。

​    为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上 了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个 新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。

每个Server在工作过程中有三种状态：

- LOOKING：当前Server不知道leader是谁，正在搜寻
- LEADING：当前Server即为选举出来的leader
- FOLLOWING：leader已经选举出来，当前Server与之同步

### 2.1 选主流程

   当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的 Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。先介绍basic paxos流程：

​    1 .选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server；

​    2 .选举线程首先向所有Server发起一次询问(包括自己)；

​    3 .选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息(    id,zxid)，并将这些信息存储到当次选举的投票记录表中；

​    \4.  收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server；

​    \5.  线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n/2 + 1的Server票数， 设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。

  通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1.

  每个Server启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的server还会从磁盘快照中恢复数据和会话信息，zk会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。选主的具体流程图如下所示：

​                     ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213346_59.png)

   fast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和 zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。其流程图如下所示：

​             ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213346_900.png)

### 2.2 同步流程

选完leader以后，zk就进入状态同步过程。

​    \1. leader等待server连接；

​    2 .Follower连接leader，将最大的zxid发送给leader；

​    3 .Leader根据follower的zxid确定同步点；

​    4 .完成同步后通知follower 已经成为uptodate状态；

​    5 .Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。

流程图如下所示：

​                 ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213347_647.jpg)

## 2.3 工作流程

### 2.3.1 Leader工作流程

Leader主要有三个功能：

​    1 .恢复数据；

​    2 .维持与Learner的心跳，接收Learner请求并判断Learner的请求消息类型；

​    3 .Learner的消息类型主要有PING消息、REQUEST消息、ACK消息、REVALIDATE消息，根据不同的消息类型，进行不同的处理。

​    PING消息是指Learner的心跳信息；REQUEST消息是Follower发送的提议信息，包括写请求及同步请求；ACK消息是 Follower的对提议的回复，超过半数的Follower通过，则commit该提议；REVALIDATE消息是用来延长SESSION有效时间。
Leader的工作流程简图如下所示，在实际实现中，流程要比下图复杂得多，启动了三个线程来实现功能。

​                 ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213347_829.png)

### 2.3.2 Follower工作流程

Follower主要有四个功能：

​    \1. 向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）；

​    2 .接收Leader消息并进行处理；

​    3 .接收Client的请求，如果为写请求，发送给Leader进行投票；

​    4 .返回Client结果。

Follower的消息循环处理如下几种来自Leader的消息：

​    1 .PING消息： 心跳消息；

​    2 .PROPOSAL消息：Leader发起的提案，要求Follower投票；

​    3 .COMMIT消息：服务器端最新一次提案的信息；

​    4 .UPTODATE消息：表明同步完成；

​    5 .REVALIDATE消息：根据Leader的REVALIDATE结果，关闭待revalidate的session还是允许其接受消息；

​    6 .SYNC消息：返回SYNC结果到客户端，这个消息最初由客户端发起，用来强制得到最新的更新。

Follower的工作流程简图如下所示，在实际实现中，Follower是通过5个线程来实现功能的。

​             ![zookeeper简介](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20141108213347_577.png)

对于observer的流程不再叙述，observer流程和Follower的唯一不同的地方就是observer不会参加leader发起的投票。

 



## [ZooKeeper典型使用场景一览](http://www.kuqin.com/system-analysis/20111120/315148.html)

​    ZooKeeper是一个高可用的分布式数据管理与系统协调框架。基于对Paxos算法的实现，使该框架保证了分布式环境中数据的强一致性，也正是基 于这样的特性，使得zookeeper能够应用于很多场景。网上对zk的使用场景也有不少介绍，本文将结合作者身边的项目例子，系统的对zk的使用场景进 行归类介绍。 值得注意的是，zk并不是生来就为这些场景设计，都是后来众多开发者根据框架的特性，摸索出来的典型使用方法。因此，也非常欢迎你分享你在ZK使用上的奇 技淫巧。

​    

| 场景类别       | 典型场景描述（ZK特性，使用方法）                             | 应用中的具体使用                                             |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据发布与订阅 | 发布与订阅即所谓的配置管理，顾名思义就是将数据发布到zk节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，地址列表等就非常适合使用。 | 1. 索引信息和集群中机器节点状态存放在zk的一些指定节点，供各个客户端订阅使用。2. 系统日志（经过处理后的）存储，这些日志通常2-3天后被清除。 3. 应用中用到的一些配置信息集中管理，在应用启动的时候主动来获取一次，并且在节点上注册一个Watcher，以后每次配置有更新，实时通知到应用，获取最新配置信息。4. 业务逻辑中需要用到的一些全局变量，比如一些消息中间件的消息队列通常有个offset，这个offset存放在zk上，这样集群中每个发送者都能知道当前的发送进度。5. 系统中有些信息需要动态获取，并且还会存在人工手动去修改这个信息。以前通常是暴露出接口，例如JMX接口，有了zk后，只要将这些信息存放到zk节点上即可。 |
| Name Service   | 这个主要是作为分布式命名服务，通过调用zk的create node api，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。 |                                                              |
| 分布通知/协调  | ZooKeeper 中特有watcher注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对 ZK上同一个znode进行注册，监听znode的变化（包括znode本身内容及子节点的），其中一个系统update了znode，那么另一个系统能 够收到通知，并作出相应处理。 | 1. 另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过zk上某个节点关联，大大减少系统耦合。2. 另一种系统调度模式：某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改 了ZK上某些节点的状态，而zk就把这些变化通知给他们注册Watcher的客户端，即推送系统，于是，作出相应的推送任务。 3. 另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到zk来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。总之，使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合。 |
| 分布式锁       | 分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性，即用户只要完全相信每时每刻，zk集群中任意节点（一个zk server）上的相同znode的数据是一定是相同的。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 所 谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把zk上的一个znode看作是一把锁，通过create znode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。控 制时序，就是所有视图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指 定）。Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。 |                                                              |
| 集群管理       | 1. 集群机器监 控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群 机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。 这种做法可行，但是存在两个比较明显的问题：1. 集群中机器有变动的时候，牵连修改的东西比较多。2. 有一定的延时。 利 用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统：a. 客户端在节点 x 上注册一个Watcher，那么如果 x 的子节点变化了，会通知该客户端。b. 创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。例 如，监控系统在 /clusterServers 节点上注册一个Watcher，以后每动态加机器，那么就往 /clusterServers 下创建一个 EPHEMERAL类型的节点：/clusterServers/{hostname}. 这样，监控系统就能够实时知道机器的增减情况，至于后续处理就是监控系统的业务了。 2. Master选举则是zookeeper中最为经典的使用场景了。在 分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行， 其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选取了。另外，这种场景演化一下，就是动态Master选举。这就要用到 EPHEMERAL_SEQUENTIAL类型节点的特性了。上 文中提到，所有客户端创建请求，最终只有一个能够创建成功。在这里稍微变化下，就是允许所有请求都能够创建成功，但是得有个创建顺序，于是所有的请求最终 在ZK上创建结果的一种可能情况是这样： /currentMaster/{sessionId}-1 , /currentMaster/{sessionId}-2 , /currentMaster/{sessionId}-3 ….. 每次选取序列号最小的那个机器作为Master，如果这个机器挂了，由于他创建的节点会马上小时，那么之后最小的那个机器就是Master了。 | 1. 在搜索系统中，如果集群中每个机器都生成一份全量索引，不仅耗时，而且不能保证彼此之间索引数据一致。因此让集群中的Master来进行全量索引的生成， 然后同步到集群中其它机器。2. 另外，Master选举的容灾措施是，可以随时进行手动指定master，就是说应用在zk在无法获取master信息时，可以通过比如http方式，向 一个地方获取master。 |
| 分布式队列     | 队列方面，我目前感觉有两种，一种是常规的先进先出队列，另一种是要等到队列成员聚齐之后的才统一按序执行。对于第二种先进先出队列，和分布式锁服务中的控制时序场景基本原理一致，这里不再赘述。 第 二种队列其实是在FIFO队列的基础上作了一个增强。通常可以在 /queue 这个znode下预先建立一个/queue/num 节点，并且赋值为n（或者直接给/queue赋值n），表示队列大小，之后每次有队列成员加入后，就判断下是否已经到达队列大小，决定是否可以开始执行 了。这种用法的典型场景是，分布式环境中，一个大任务Task A，需要在很多子任务完成（或条件就绪）情况下才能进行。这个时候，凡是其中一个子任务完成（就绪），那么就去 /taskList 下建立自己的临时时序节点（CreateMode.EPHEMERAL_SEQUENTIAL），当 /taskList 发现自己下面的子节点满足指定个数，就可以进行下一步按序进行处理了。 |                                                              |

 

 

##  Zookeeper 节点类型 & 节点信息

> zookeeper 从两个纬度将ZNode 节点划分为四种类型: 持久有序型, 持久无序型, 短暂有序型, 短暂无序型. 从持久化角度来看, 当客户端断开连接后, 持久型节点不会消失, 而短暂性节点会消失; 从有序性角度来看, 有序节点创建时, 会自动为节点名称添加序号, 而无序节点创建的节点名称就是指定的节点名称.

## 1. 节点类型

### 1.1 有序性验证

- 有序节点名称并非指定的名称, 而会被添加序号
- 序号生成规则: 为以十位数格式的父节点节点信息中dataLength中的值

```
# mynodes 节点下当前拥有两个节点
[zk: localhost:2181(CONNECTED) 4] ls /mynodes
[A, B]

# 创建有序节点C, 序号为当前父节点/mynodes节点信息中的dataLength 的值, 而非从1开始. 数字是一个十位数的格式
[zk: localhost:2181(CONNECTED) 5] create -s /mynodes/C ""
Created /mynodes/C0000000002

#　查看所有节点
[zk: localhost:2181(CONNECTED) 6] ls /mynodes
[A, B, C0000000002]


```

### 1.2 持久型验证

- 短暂型节点创建时, 需要使用使用 -e 选项
- 测试时, 需要断开客户端连接, 重新连接.

```
# 创建父节点/mynodes
[zk: localhost:2181(CONNECTED) 0] create /mynodes ""
Created /mynodes

# 创建两个持久性节点,　创建两个段暂性节点
[zk: localhost:2181(CONNECTED) 1] create /mynodes/p-node-1 ""
Created /mynodes/p-node-1
[zk: localhost:2181(CONNECTED) 2] create /mynodes/p-node-2 ""
Created /mynodes/p-node-2
[zk: localhost:2181(CONNECTED) 3] create -e /mynodes/e-node-1 ""
Created /mynodes/e-node-1
[zk: localhost:2181(CONNECTED) 4] create -e /mynodes/e-node-2 ""
Created /mynodes/e-node-2

# 查看当前节点列表
[zk: localhost:2181(CONNECTED) 5] ls /mynodes
[p-node-2, e-node-2, e-node-1, p-node-1]

# 端开客户端连接
[zk: localhost:2181(CONNECTED) 6] quit

断开客户端连接后, 查看节点信息, 只剩下了持久性节点.

$ ./zkCli.sh ls /mynodes
[p-node-2, p-node-1]
```

## 2. 节点信息

ZNode 除了存储用户设置的数据之外, 还存储着一些节点的信息, 如版本号, 修改时间等信息, 称之为节点信息.

| 字段           | 描述                                       |
| -------------- | ------------------------------------------ |
| czxid          | 节点被创建时的事务id                       |
| ctime          | 节点上次修改的事务id                       |
| mzxid          | 节点上次修改的事务id                       |
| mtime          | 节点上次修改的时间                         |
| dataversion    | 节点被修改的版本号                         |
| datalengh      | 节点存储的数据的长度, 单位字节             |
| aclversion     | 节点的ACL 被修改的版本号                   |
| ephemeralOwner | 临时节点的拥有者的sessionid, 持久性节点为0 |
| numChildren    | 子节点的数量                               |
| cversion       | 子节点变化版本号, 新增/删除子节点时会自增  |
| pzxid          | 子节点点最近修改(新增/删除)的zxid          |

set /mynodes "this is mynodes"

 create /mynodes "mynodes"

get /mynodes

```
# 创建持久化节点
[zk: localhost:2181(CONNECTED) 2] create /mynodes "mynodes"
Created /mynodes
[zk: localhost:2181(CONNECTED) 3] get /mynodes
# 节点存储数据
mynodes
# 节点创建时事务id和时间, 一旦创建,则不再改变
cZxid = 0x1e4
ctime = Sun Feb 17 15:23:44 CST 2019
# 节点修改时事务id和时间, 修改节点数据内容时变更
mZxid = 0x1e4
mtime = Sun Feb 17 15:23:44 CST 2019

# 子节点数量变更事务id, 新增或删除子节点时影响.
pZxid = 0x1e4
# 子节点变化版本号, 新增或删除子节点时影响
cversion = 0
# 节点数据版本号, 修改节点数据内容时会变更, 初始化为0
dataVersion = 0
# 节点acl权限版本号, 初始化为0
aclVersion = 0
# 持久化节点为0x0, 非持久化节点为sessionid, 如: 0x100092d5d8b0018
ephemeralOwner = 0x0
# 节点数据长度
dataLength = 7
# 子节点数量
numChildren = 0


```

### 2.3 创建子节点

新增子节点, 会导致 cversion, pZxid, numChildren 发生变化.

```bash
[zk: localhost:2181(CONNECTED) 5] create -e /mynodes/A ""
Created /mynodes/A
[zk: localhost:2181(CONNECTED) 6] get /mynodes                  
this is mynodes
cZxid = 0x1e4
ctime = Sun Feb 17 15:23:44 CST 2019
mZxid = 0x1e6
mtime = Sun Feb 17 15:24:12 CST 2019
# 发生变化
pZxid = 0x1e7
# 发生变化
cversion = 1
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 15
# 发生变化
numChildren = 1
```

### pom 依赖

引入zookeeper 核心依赖, 顺便引入单元测试和日志依赖.

```markdown
<dependencies>
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.12</version>
    </dependency>
    <dependency>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
        <version>1.2.17</version>
    </dependency>
    <dependency>
        <groupId>org.apache.zookeeper</groupId>
        <artifactId>zookeeper</artifactId>
        <version>3.4.6</version>
    </dependency>
</dependencies>

```

## 2. 创建Zookeeper 连接

Java 对Zookeeper 的操作全部封装在Zookeeper 对象中, 需要先创建Zookeeper对象, 与Zookeeper服务建立连接. 获取到Zookeeper 连接对象之后, 就可以对zNode 进行操作了.

```java
public class TestAPI {

    // zookeeper 地址, 多个地址以逗号割开
    private static String zkServer = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183";
    
    // 会话超时时间
    private static int sessionTimeout = 3000;
    
    // zookeeper 客户端连接
    private static ZooKeeper zooKeeper;
    
    @Test
    public void init() throws Exception{
        zooKeeper = new ZooKeeper(zkServer, sessionTimeout, watchedEvent ->{});
        System.out.println("******************* init start ***********************");
        System.out.println("state:" + zooKeeper.getState());
        System.out.println("sessionPwd:" + zooKeeper.getSessionPasswd());
        System.out.println("sessionId:" + zooKeeper.getSessionId());
        System.out.println("timeout:" + zooKeeper.getSessionTimeout());
    
        System.out.println("******************* init end  ***********************");
    }
}
```

## Zookeeper JavaAPI-Watcher监听节点变化

### 1. Zookeeper监听

1.1 监听器类型

从监听器范围来区分, Zookeeper 监听器分为两种, 一种是全局监听器,一种是局部监听器.

- 全局监听器: 创建Zookeeper 连接时指定, 全局唯一. 一旦注册不能删除, 默认无限监听事件.
- 局部监听器: 对节点进行操作时, 指定具体场景的监听器. 默认一次注册监听, 只能处理一次事件触发.

### 1.2 事件类型

Zookeeper 监听的事件类型有四种:

- EventType.NodeCreated: 节点创建事件
- EventType.NodeDeleted: 节点删除事件
- EventType.NodeDataChanged: 节点数据内容变更
- EventType.NodeChildrenChanged: 子节点数量发生变化, 新增或删除子节点. 子节点内容发生变化, 不会触发.

## 2. 使用全局监听器监听

- 一条Zookeeper 连接只有一个全局监听器, 在创建Zookeeper对象时指定
- 全局监听器中监听的事件, 只能注册, 不能删除. 除非断开连接, 重新连接. 因此要慎用
- 全局监听器中监听的事件, 会循环监听. 并非触发一次就终止了.
- 全局监听器中需要根据节点路径和事件类型综合判断来做相应的业务逻辑.

### 2.1 全局监听器API

- 使用全局监听器的API 都会传一个boolean 类型为true的参数, 声明此操作需要监听, 会在全局监听器中注册一条监听信息, 但不能删除.
- 当监听的事件发生后, 会触发全局监听器进行处理. 因此需要在全局监听器中根据节点路径和事件类型做判断.

```java
public byte[] getData(final String path, Boolean watcher, Stat stat);
public List<String> getChildren(final String path, Boolean watcher);
public Stat exists(final String path, Boolean watcher);
....
1234
```

### 2.2 测试用例

```java
public class TestWatch {

    // zookeeper 地址, 多个地址以逗号割开
    private static String zkServer = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183";

    // 会话超时时间
    private static int sessionTimeout = 3000;

    // zookeeper 客户端连接
    private static ZooKeeper zooKeeper;

    @BeforeClass
    public static void init() throws Exception{
        // 创建zookeeper 连接时, 设置全局监听器
        zooKeeper = new ZooKeeper(zkServer, sessionTimeout, watchedEvent ->{
            try {
                // 获取发生事件的节点路径
                String path = watchedEvent.getPath();

                // 根据节点路径和事件类型做不同的业务的处理
                if ("/".equals(path) && Watcher.Event.EventType.NodeChildrenChanged.equals(watchedEvent.getType())) {

                    // 获取数据, 并继续监控
                    List<String> children = zooKeeper.getChildren("/", true);
                    System.out.println("系统主监控-根节点/下的子节点: " + children);
                }


            } catch (KeeperException e) {
                e.printStackTrace();
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        });
    }

    // 获取节点时, 不指定监控器Watcher ,使用默认的Watcher
    // 获取节点时, 声明需要监控节点数量, 但并为指定监控器. 因此, 子节点数量发生变化时, 会交由创建Zookeeper 连接时, 设置的Watcher 处理
    // getChildren 只注册监听NodeChildrenChanged 事件, 即当自节点数量发生变化时, 会通知.
    @Test
    public void test_getChildren() throws KeeperException, InterruptedException {

        // 一旦注册便会一直监听,只要有变化, 就会通知全局监听器
        zooKeeper.getChildren("/", true);

        // 线程休眠, 否则不能监控到数据
        Thread.sleep(Long.MAX_VALUE);
    }
}
```

### 2.3 使用Shell 客户端添加节点

```bash
[zk: 127.0.0.1:2181(CONNECTED) 84] create /h ""
Created /h
[zk: 127.0.0.1:2181(CONNECTED) 85] create /hh ""
Created /hh
1234
```

### 2.4 控制台输出

新增一个节点, 打印一条日志.

```bash
系统主监控-根节点/下的子节点: [zookeeper, h]
系统主监控-根节点/下的子节点: [zookeeper, hh]
12
```

## 3. 局部监听器

### 3.1 局部监听API

- 局部监听器都需要传入一个Watcher 类型的监听器.
- 当监听的事件发生后, 会同自定义的Watcher处理.
- 默认情况下, 一次监听注册只能处理一次事件变更. 如果需要持续监听, 则需要在处理逻辑中再次监听.

```java
public byte[] getData(final String path, Watcher watcher, Stat stat);
public List<String> getChildren(final String path, Watcher watcher);
public Stat exists(final String path, Watcher watcher);
....
1234
```

### 3.2 测试用例

```java
import org.apache.zookeeper.*;
import org.apache.zookeeper.data.Stat;
import org.junit.BeforeClass;
import org.junit.Test;

import java.util.List;

/**
 * @Description: 测试监听
 * @author: zongf
 * @date: 2019-02-17 10:20
 */
public class TestWatch {

    // zookeeper 地址, 多个地址以逗号割开
    private static String zkServer = "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183";

    // 会话超时时间
    private static int sessionTimeout = 3000;

    // zookeeper 客户端连接
    private static ZooKeeper zooKeeper;

    @BeforeClass
    public static void init() throws Exception{
        // 创建zookeeper 连接时, 设置全局监听器
        zooKeeper = new ZooKeeper(zkServer, sessionTimeout, watchedEvent ->{});
    }

   
    // 获取节点时, 指定监控器Watcher. 当有事件发生时, 会由自定义的Watcher 处理而非系统监控器处理
    // getChildren 指定的监控器Watcher 只能注册监控odeChildrenChanged事件.
    @Test
    public void test_getChildren_Watch() throws KeeperException, InterruptedException {

        final String nodePath = "/";

        List<String> children = zooKeeper.getChildren(nodePath, new Watcher() {
            @Override
            public void process(WatchedEvent event) {
                // 获取数据, 并继续监控. this 表示将监控器本身作为监控器继续监控
                try {
                    List<String> childs = zooKeeper.getChildren(nodePath, this);
                    System.out.println("自定义节点监控-根节点/下的子节点: " + childs + ", event-type:" + event.getType());
                } catch (KeeperException e) {
                    e.printStackTrace();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }

            }
        });

        // 线程休眠, 否则不能监控到数据
        Thread.sleep(Long.MAX_VALUE);
    }


    // 获取数据时, 添加监控, 只能监控NodeDataChanged 事件. 只能监控一次
    @Test
    public void test_getData_watch() throws KeeperException, InterruptedException {
        final String nodePath = "/node-anyone";

        zooKeeper.getData(nodePath, new Watcher() {
            @Override
            public void process(WatchedEvent event) {
                try {
                    // this表示获取节点数据时再次使用此watcher监控, 这样之后节点数据再发生变化,依然可以监控到
                    byte[] data = zooKeeper.getData(nodePath, this, new Stat());
                    System.out.println("event:" + event + ", data:" + new String(data));
                } catch (Exception e) {
                    e.printStackTrace();
                }
            }
        }, new Stat());

        // 线程休眠, 否则不能监控到数据
        Thread.sleep(Long.MAX_VALUE);
    }

    // 监控节点是否存在, 只监控一次.
    @Test
    public void test_exsists_watch() throws KeeperException, InterruptedException {
        zooKeeper.exists("/host1", new Watcher() {
            @Override
            public void process(WatchedEvent event) {

                System.out.println("节点:/host1 存在, event-type:" + event.getType());
            }
        });

        // 线程休眠, 否则不能监控到数据
        Thread.sleep(Long.MAX_VALUE);
    }
}
```

## JavaAPI-常用操作

### 1. 创建节点API

#### 1.1 创建默认权限节点

```java
zooKeeper.create("/node-anyone", "test node anyone".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
1
```

#### 1.2 创建auth 权限模式节点

```java
// 添加用户认证信息
zooKeeper.addAuthInfo("digest", "admin:admin".getBytes());
zooKeeper.addAuthInfo("digest", "admin:123456".getBytes());

// 创建节点
zooKeeper.create("/node-auth", "test node auth".getBytes(), ZooDefs.Ids.CREATOR_ALL_ACL, CreateMode.PERSISTENT);

```

### 1.3 创建digest 模式权限节点

```java
 // 权限位使用或运算符拼接
int aclVal = ZooDefs.Perms.CREATE | ZooDefs.Perms.DELETE | ZooDefs.Perms.READ | ZooDefs.Perms.WRITE;

// 生成加密的用户认证信息
String authentication = DigestAuthenticationProvider.generateDigest("admin:admin");

// 创建权限模式
ACL acl = new ACL(aclVal, new Id("digest", authentication));

// 创建节点
zooKeeper.create("/node-digest", "test node auth".getBytes(), Collections.singletonList(acl), CreateMode.PERSISTENT);
1234567891011
```

### 1.4 创建短暂有序节点

```java
 // 创建节点, 返回节点名称. 因为有序, 所以节点名称和指定名称不同
String nodeName = zooKeeper.create("/node-es", "test node anyone".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
System.out.println("nodeName:" + nodeName);
123
```

## 2. 节点操作API

### 2.1 获取节点数据

```java
 // 用于保存节点信息
Stat stat = new Stat();

// 获取节点数据
byte[] data = zooKeeper.getData("/node-anyone", false, stat);
System.out.println("data:" + new String(data));
System.out.println("stat.version:" + stat.getVersion());
1234567
```

### 2.2 获取子节点

```java
List<String> children = zooKeeper.getChildren("/", false);
1
```

### 2.3 判断节点是否存在

```java
Stat stat = zooKeeper.exists("/sss", false);
1
```

### 2.4 删除节点

删除节点时, 必须指定节点版本号.

```java
 // 获取节点状态
Stat stat = new Stat();
zooKeeper.getData("/test", false, stat);

// 删除节点
zooKeeper.delete("/test", stat.getVersion());
123456
```

### 3. 权限操作API

#### 3.1 获取权限

```java
List<ACL> aclList = zooKeeper.getACL("/node-auth", new Stat());
1
```

#### 3.2 设置权限

设置权限时, 需要指定节点版本号.

```java
// 创建节点信息对象, 用于保存节点信息
Stat nodeStat = new Stat();
zooKeeper.getData(nodePath, false, nodeStat);

// 更新节点权限
zooKeeper.setACL("/node-auth", ZooDefs.Ids.READ_ACL_UNSAFE, nodeStat.getVersion());
123456
```

### 4. 批量操作

zookeeper 提供了multi() 和 transaction()API 来进行批量操作. 而Transaction 对象其实就是将multi 进行了一层封装而已.

```java
//批量操作
List<Op> opList = new ArrayList<>();
opList.add(Op.create("/nodes", "nodes".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT));
opList.add(Op.create("/nodes/A", "aa".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT));
opList.add(Op.create("/nodes/B", "bb".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT));
opList.add(Op.create("/nodes/C", "cc".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT));
//        如果添加此行, 则全部失败
//        opList.add(Op.create("/nodes/D/dd", "dd".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT));


List<OpResult> multi = zooKeeper.multi(opList);
for (OpResult opResult : multi) {

}
multi.stream()
        .map(opResult -> (OpResult.CreateResult) opResult)
        .forEach(result -> System.out.println(result.getPath() + "-> " + result.getType()));
```

## 11. Zookeeper JavaAPI-异步调用

## 1. 异步API 简介

Zookeeper 中几乎对常用的API都提供了异步调用方式, 所有异步API都有以下特点:

- 返回均为void, 因为异步执行, 返回值并没有任何意义
- 都需要实现回调方法, 回到方法参数也很类似
- 都需要设置一个回调参数了 ctx, 参数会原封不动地会传到回调方法

```java
public void create(final String path, byte data[], List<ACL> acl, CreateMode createMode,  StringCallback cb, Object ctx)
public void delete(final String path, int version, VoidCallback cb, Object ctx) 
public void setData(final String path, byte data[], int version, StatCallback cb, Object ctx)
public void setACL(final String path, List<ACL> acl, int version, StatCallback cb, Object ctx)
public void getData(final String path, Watcher watcher, DataCallback cb, Object ctx)            
public void getChildren(final String path, Watcher watcher, ChildrenCallback cb, Object ctx)            
public void getACL(final String path, Stat stat, ACLCallback cb, Object ctx)           
public void exists(final String path, Watcher watcher, StatCallback cb, Object ctx) 
```

https://blog.csdn.net/zongf0504/article/details/87684246

## 12. Zookeeper 实战一: 监控节点动态上下线

https://blog.csdn.net/zongf0504/article/details/89276453

利用Zookeeper 临时型节点特性(当断开连接时, 创建的临时性节点自动销毁), 可实现监听集群节点动态上下线的功能. 所谓监听监听节点动态上下线是指, 当有节点新增或删除时, 监听端都会收到信息. zookeeper 实现的监听节点动态上下线特点:

- 通过向节点注册监听器, 来监听该节点子节点的创建和删除
- 当子节点有创建和删除操作时, 监听端会接收到事件, 但是却不知道具体是新增或删除哪个节点, 需要重新查询所有子节点数据, 来进行更新列表
- 使用局部监听器时,需要注意局部监听器只能监听一次事件. 当监听到事件后, 需要将监听器自身再次注册为监听器, 才能保证继续监听
- zk 监听类似于广播, 当一个节点注册了多个监听器时, 当有事件发生时, 事件会通知给所有监听器

## 单机搭建zookeeper伪集群

https://blog.csdn.net/vbirdbest/article/details/82688462

# 4.kafka

Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。

分布式 基于发布订阅模式的消息队列

## 1.Kafka和RabbitMQ的异同点和使用场景



### 1.架构模型

​		RabbitMQ遵循AMQP协议，RabbitMQ的broker由Exchange,Binding,queue组成，其中exchange和binding组成了消息的路由键；客户端Producer通过连接channel和server进行通信，Consumer从queue获取消息进行消费（长连接，queue有消息会推送到consumer端，consumer循环从输入流读取数据）。rabbitMQ以broker为中心。

​		Kafka遵从一般的MQ结构，producer，broker，consumer，以consumer为中心，消息的消费信息保存的客户端consumer上，consumer根据消费的点，从broker上批量pull数据。

 2.消息确认机制

  rabbitmq:具有生产者confirm机制以及消费者的消息应答机制ack。

  Kafka:不具有应答机制。

  3.消息的顺序

  rabbitmq:在一个队列里面，rabbitmq的消息是严格顺序的，按照先进先出。

  kafka:  在同一个partition中消息是有序的，但是生产者put到Kafka中数据会分布在不同的partition中，所有总体是无序的。

  4.吞吐量

  rabbitmq: 根据测试，RabbitMQ在不使用ACK机制的，Msg大小为1K的情况下，QPS可达6W＋。再双方ACK机制，Msg大小为1K的情况下，QPS瞬间降到了1W＋。

  Kafka:Kafka具有巨大的吞吐量，数据的存储以及获取是本地磁盘的批量处理，可以达到百万/s。

  5.可靠性

 RabbitMQ使用了MirrorQueue的机制，也可以做到多个机器进行热备。

  Kafka的broker支持主备模式。

  7.持久化

  rabbitmq

  支持

  kafka

  Kafka 是一个持久性消息存储。

   

  对于他们的使用场景如下

  rabbitmq    

`1.RabbitMQ的消息应当尽可能的小，并且只用来处理实时且要高可靠性的消息。`

`2.消费者和生产者的能力尽量对等，否则消息堆积会严重影响RabbitMQ的性能。`

`3.集群部署，使用热备，保证消息的可靠性。`

  kafka 

1. `1.应当有一个非常好的运维监控系统，不单单要监控Kafka本身，还要监控Zookeeper。(kafka强烈的依赖于zookeeper,如果zookeeper挂掉了，那么Kafka也不行了)`
2. `2.对消息顺序不依赖，且不是那么实时的系统。`
3. `3.对消息丢失并不那么敏感的系统。`
4. `4.从 A 到 B 的流传输，无需复杂的路由，最大吞吐量可达每秒 100k 以上。`



## 2.Kafka介绍

Kafka是由LinkedIn开发的一个分布式基于发布/订阅的消息系统，使用Scala编写，它以可水平扩展和高吞吐率而被广泛使用。
Kafka是什么？举个例子，生产者消费者，生产者生产鸡蛋，消费者消费鸡蛋，生产者生产一个鸡蛋，消费者就消费一个鸡蛋，假设消费者消费鸡蛋的时候噎住了（系统宕机了），生产者还在生产鸡蛋，那新生产的鸡蛋就丢失了。再比如生产者很强劲（大交易量的情况），生产者1秒钟生产100个鸡蛋，消费者1秒钟只能吃50个鸡蛋，那要不了一会，消费者就吃不消了（消息堵塞，最终导致系统超时），消费者拒绝再吃了，”鸡蛋“又丢失了，这个时候我们放个篮子在它们中间，生产出来的鸡蛋都放到篮子里，消费者去篮子里拿鸡蛋，这样鸡蛋就不会丢失了，都在篮子里，而这个篮子就是”kafka“。鸡蛋其实就是“数据流”，系统之间的交互都是通过“数据流”来传输的（就是tcp、http什么的），也称为报文，也叫“消息”。消息队列满了，其实就是篮子满了，”鸡蛋“放不下了，那赶紧多放几个篮子，其实就是kafka的扩容。
就类似微博，有人发布消息，有人消费消息，这就是一个Kafka的场景。

### Kafka和其他主流分布式消息系统的对比

![这里写图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20171123150520142)

3个中只有ActiveMQ支持，这个是因为，RabbitMQ和Kafka为了更高的性能，而放弃了对事物的支持 。

、动态扩容是很多公司要求的技术之一，不支持动态扩容就意味着停止服务，这对很多公司来说是不可以接受的。

、Java 和 scala都是运行在JVM上的语言。
、erlang和最近比较火的和go语言一样是从代码级别就支持高并发的一种语言，所以RabbitMQ天生就有很高的并发性能，但是 有RabbitMQ严格按照AMQP进行实现，受到了很多限制。kafka的设计目标是高吞吐量，所以kafka自己设计了一套高性能但是不通用的协议，他也是仿照AMQP（ Advanced Message Queuing Protocol 高级消息队列协议）设计的。

Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。**1.1  Kafka的特性:**

\- 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition, consumer group 对partition进行consume操作。

\- 可扩展性：kafka集群支持热扩展

\- 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失

\- 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）

\- 高并发：支持数千个客户端同时读写

 

**1.2  Kafka的使用场景：**

\- 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。

\- 消息系统：解耦和生产者和消费者、缓存消息等。

\- 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。

\- 运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。

\- 流式处理：比如spark streaming和storm

\- 事件源

## 3.相关概念

1）Producer ：消息生产者，就是向kafka broker 发消息的客户端；
2）Consumer ：消息消费者，向kafka broker 取消息的客户端；
3）Consumer Group （CG）：消费者组，由多个consumer 组成。消费者组内每个消费者负
责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所
有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。
4）Broker ：一台kafka 服务器就是一个broker。一个集群由多个broker 组成。一个broker
可以容纳多个topic。
5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；
6）Partition：为了实现扩展性，一个非常大的topic 可以分布到多个broker（即服务器）上，
一个topic 可以分为多个partition，每个partition 是一个有序的队列；
7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition 数据不丢失，且kafka 仍然能够继续工作，kafka 提供了副本机制，一个topic 的每个分区都有若干个副本，
一个leader 和若干个follower。 
8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。 
9）follower：每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据的同步。leader 发生故障时，某个 follower 会成为新的 follower。 

**Topics 和Logs**

先来看一下Kafka提供的一个抽象概念:topic.
一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区，如下图所示：
![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/225851kqq1pnqbq81kblln.png) 

每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。
在一个可配置的时间段内，Kafka集群保留所有发布的消息，不管这些消息有没有被消费。比如，如果消息的保存策略被设置为2天，那么在一个消息被发布的两天时间内，它都是可以被消费的。之后它将被丢弃以释放空间。Kafka的性能是和数据量无关的常量级的，所以保留太多的数据并不是问题。

实际上每个consumer唯一需要维护的数据是消息在日志中的位置，也就是offset.这个offset有consumer来维护：一般情况下随着consumer不断的读取消息，这offset的值不断增加，但其实consumer可以以任意的顺序读取消息，比如它可以将offset设置成为一个旧的值来重读之前的消息。

以上特点的结合，使Kafka consumers非常的轻量级：它们可以在不对集群和其他consumer造成影响的情况下读取消息。你可以使用命令行来"tail"消息而不会对其他正在消费消息的consumer造成影响。

将日志分区可以达到以下目的：首先这使得每个日志的数量不会太大，可以在单个服务上保存。另外每个分区可以单独发布和消费，为并发操作topic提供了一种可能。

![04.png](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/04.png)

## 4.集群部署 

tar -zxvf kafka_2.11-0.11.0.0.tgz -C  /opt/module/ 

修改解压后的文件名称 mv kafka_2.11-0.11.0.0/  kafka

在/opt/module/kafka 目录下创建logs 文件夹
 $ mkdir logs 
4）修改配置文件
 $ cd config/ 
 $ vi server.properties 
输入以下内容： 
#broker的全局唯一编号，不能重复
broker.id=0 
#删除topic功能使能
delete.topic.enable=true 

#处理网络请求的线程数量
num.network.threads=3 
#用来处理磁盘IO的现成数量
num.io.threads=8 
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400 
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400 
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600 
#kafka运行日志存放的路径
log.dirs=/opt/module/kafka/logs 
#topic在当前broker上的分区个数
num.partitions=1 
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1 
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168 
#配置连接Zookeeper集群地址
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181 
5）配置环境变量
[atguigu@hadoop102 module]$ sudo vi /etc/profile 
#KAFKA_HOME 
export KAFKA_HOME=/opt/module/kafka 
export PATH=$PATH:$KAFKA_HOME/bin 
[atguigu@hadoop102 module]$ source /etc/profile 
6）分发安装包

xsync kafka/ 注意：分发之后记得配置其他机器的环境变量 

7）分别在 hadoop103 和 hadoop104 上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2   。注：broker.id 不得重复 

8）启动集群

依次在hadoop102、hadoop103、hadoop104 节点上启动 kafka

@hadoop102 kafka  $ bin/kafka-server-start.sh -daemon config/server.properties

9）关闭集群
[atguigu@hadoop102 kafka]$ bin/kafka-server-stop.sh stop [atguigu@hadoop103 kafka]$ bin/kafka-server-stop.sh stop [atguigu@hadoop104 kafka]$ bin/kafka-server-stop.sh stop 
10）kafka 群起脚本
for i in hadoop102 hadoop103 hadoop104 
do 
echo "========== $i =========="  
ssh $i '/opt/module/kafka/bin/kafka-server-start.sh -daemon
/opt/module/kafka/config/server.properties' 
done 

![image-20201109134638382](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/image-20201109134638382.png)

这里如果群起不行，需要在执行命令前加入source /etc/profile

## 05.Kafka入门_安装&启动&关闭

### Quick Start

本次安装学习在Windows操作系统进行。（Linux版本的差别不大，运行脚本文件后缀从`bat`改为`sh`，配置路径改用Unix风格的）

[本节教程来源](http://kafka.apache.org/0110/documentation/#quickstart)

#### Step 1: Download the code

[下载代码](http://kafka.apache.org/downloads)并解压

下载[kafka 0.11.0.0](https://archive.apache.org/dist/kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz)版本，解压到`C:\Kafka\`路径下，Kafka主目录文件为`C:\Kafka\kafka_2.11-0.11.0.0`（下文用KAFKA_HOME表示）。

#### Step 2: Start the server

Kafka 用到 ZooKeeper 功能，所以要预先运行ZooKeeper。了解更多ZooKeeper信息，可点击阅读[ZooKeeper学习笔记](https://my.oschina.net/jallenkwong/blog/4405741)。

- 首先，修改`%KAFKA_HOME%\conf\zookeeper.properties`中的`dataDir=/tmp/zookeeper`，改为`dataDir=C:\\Kafka\\data\\zookeeper`。
- 创建新目录`C:\\Kafka\\data\\zookeeper`。
- 启动cmd，工作目录切换到`%KAFKA_HOME%`，执行命令行：

```bat
start bin\windows\zookeeper-server-start.bat config\zookeeper.properties
```

- 修改`%KAFKA_HOME%\conf\server.properties`中的`log.dirs=/tmp/kafka-logs`，改为`log.dirs=C:\\Kafka\\data\\kafka-logs`。
- 创建新目录`C:\\Kafka\\data\\kafka-logs`。
- 另启动cmd，工作目录切换到`%KAFKA_HOME%`，执行命令行：

```bat
start bin\windows\kafka-server-start.bat config\server.properties
```

- 可写一脚本，一键启动
- 关闭服务，`bin\windows\kafka-server-stop.bat`和`bin\windows\zookeeper-server-stop.bat`。

------

TODO:**一个问题**，通过`kafka-server-stop.bat`或右上角关闭按钮来关闭Kafka服务后，马上下次再启动Kafka，抛出异常，说某文件被占用，需清空`log.dirs`目录下文件，才能重启Kafka。

```
[2020-07-21 21:43:26,755] ERROR There was an error in one of the threads during logs loading: java.nio.file.FileSystemException: C:\Kafka\data\kafka-logs-0\my-replicated-topic-0\00000000000000000000.timeindex: 另一个程序正在使用此文件，进程无法访问。
 (kafka.log.LogManager)
...
```

参阅网络，这可能是在windows下的一个Bug，没有更好的解决方案，暂时写个py脚本用来对kafka的log文件进行删除。下次启动kafka，先运行这个删除脚本吧。

**好消息**，当你成功启动kafka，然后在对应的命令行窗口用`Ctrl + C`结束Kakfa，下次不用清理kafka日志，也能正常启动。

#### Step 3: Create a topic

- 用单一partition和单一replica创建一个名为`test`的topic:

```bat
bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
```

- 查看已创建的topic，也就刚才创建的名为`test`的topic：

```bat
bin\windows\kafka-topics.bat --list --zookeeper localhost:2181
```

或者，你可配置你的broker去自动创建未曾发布过的topic，代替手动创建topic

#### Step 4: Send some messages

运行producer，然后输入几行文本，发至服务器：

```bat
bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic test
>hello, kafka.
>what a nice day!
>to be or not to be. that' s a question.
```

请勿关闭窗口，下面步骤需要用到

#### Step 5: Start a consumer

运行consumer，将[Step 4](https://my.oschina.net/jallenkwong/blog/4449224#)中输入的几行句子，标准输出。

```bat
bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test --from-beginning
hello, kafka.
what a nice day!
to be or not to be. that' s a question.
```

若你另启cmd，执行命令行`bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test --from-beginning`来运行consumer，然后在[Step 4](https://my.oschina.net/jallenkwong/blog/4449224#)中producer窗口输入一行句子，如`I must admit, I can't help but feel a twinge of envy.`，两个consumer也会同时输出`I must admit, I can't help but feel a twinge of envy.`。

#### Step 6: Setting up a multi-broker cluster

目前为止，我们仅作为一个单一broker，这不好玩。让我们弄个有三个节点的集群来玩玩。

- 首先，在`%KAFKA%\config\server.properties`的基础上创建两个副本`server-1.properties`和`server-2.properties`。

```bat
copy config\server.properties config\server-1.properties
copy config\server.properties config\server-2.properties
```

- 打开副本，编辑如下属性

```properties
#config/server-1.properties:
broker.id=1
listeners=PLAINTEXT://127.0.0.1:9093
log.dir=C:\\Kafka\\data\\kafka-logs-1
 
#config/server-2.properties:
broker.id=2
listeners=PLAINTEXT://127.0.0.1:9094
log.dir=C:\\Kafka\\data\\kafka-logs-2
```

这个`broker.id`属性是集群中每个节点的唯一永久的名称。

我们必须重写端口和日志目录，只是因为我们在同一台机器上运行它们，并且我们希望阻止brokers试图在同一个端口上注册或覆盖彼此的数据。

- 我们已经启动了Zookeeper和我们的单个节点，所以我们只需要启动两个新节点：

```bat
start bin\windows\kafka-server-start.bat config\server-1.properties
start bin\windows\kafka-server-start.bat config\server-2.properties
```

- 创建一个replication-factor为3的topic:

```bat
bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic
```

- OK，现在我们有了一个集群，但是我们怎么知道哪个broker在做什么呢？那就运行`describe topics`命令：

```bat
bin\windows\kafka-topics.bat --describe --zookeeper localhost:2181 --topic my-replicated-topic

Topic:my-replicated-topic       PartitionCount:1        ReplicationFactor:3
Configs:
		Topic: my-replicated-topic      Partition: 0    Leader: 0       
		Replicas: 0,1,2        Isr: 0,1,2
```

- 以下是输出的说明。第一行给出所有Partition的摘要，每一行提供有关一个Partition的信息。因为这个Topic只有一个Partition，所以只有一行。
  - "leader"是负责给定Partition的所有读写的节点。每个节点都可能成为Partition随机选择的leader。
  - "replicas"是复制此Partition日志的节点列表，无论它们是leader还是当前处于存活状态。
  - "isr"是一组 "in-sync" replicas。这是replicas列表的一个子集，它当前处于存活状态，并补充leader。

注意，**在我的示例中，node 0是Topic唯一Partition的leader**。（下面操作需要用到）

- 让我们为我们的新Topic发布一些信息：

```bat
bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic my-replicated-topic

>There's sadness in your eyes, I don't want to say goodbye to you.
>Love is a big illusion, I should try to forget, but there's something left in m
y head.
>
```

- 让我们接收刚刚发布的信息吧！

```bat
bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic

There's sadness in your eyes, I don't want to say goodbye to you.
Love is a big illusion, I should try to forget, but there's something left in my head.
```

- 让我们测试一下容错性，由上文可知，Broker 0 身为 leader，因此，让我们干掉它吧：
  - 先找出 Broker 0 的进程pid。
  - 杀掉 Broker 0 的进程。

```bat
wmic process where "caption='java.exe' and commandline like '%server.properties%'" get processid,caption
Caption   ProcessId
java.exe  7528

taskkill /pid 7528 /f
成功: 已终止 PID 为 7528 的进程。
```

- 原leader已被替换成它的flowers中的其中一个，并且 node 0 不在 in-sync replica 集合当中。

```bat
bin\windows\kafka-topics.bat --describe --zookeeper localhost:2181 --topic my-replicated-topic

Topic:my-replicated-topic       PartitionCount:1        ReplicationFactor:3
Configs:
		Topic: my-replicated-topic      Partition: 0    Leader: 1       Replicas: 0,1,2 Isr: 1,2
```

- 尽管原leader已逝，当原来消息依然可以接收。（注意，参数`--bootstrap-server localhost:9093`，而不是`--bootstrap-server localhost:9092`）

```bat
bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9093 --from-beginning --topic my-replicated-topic

There's sadness in your eyes, I don't want to say goodbye to you.
Love is a big illusion, I should try to forget, but there's something left in my head.
I don't forget the way your kissing, the feeling 's so strong which is lasting for so long.
```

### server.properties一瞥

```properties
#broker 的全局唯一编号，不能重复
broker.id=0
#删除 topic 功能使能
delete.topic.enable=true
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘 IO 的现成数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka 运行日志存放的路径
log.dirs=/opt/module/kafka/logs
#topic 在当前 broker 上的分区个数
num.partitions=1
#用来恢复和清理 data 下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment 文件保留的最长时间，超时将被删除
log.retention.hours=168
#配置连接 Zookeeper 集群地址
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181
```

## 07.Kafka入门_命令行操作Topic增删查

### 查看当前服务器中的所有 topic

```bat
bin\windows\kafka-topics.bat --list --zookeeper localhost:2181
```

### 创建 topic

```
bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic
```

选项说明：

- --topic 定义 topic 名
- --replication-factor 定义副本数
- --partitions 定义分区数

> 为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列；
>
> a broker = a kafka server a broker can contain N topic a topic can contain N partition a broker can contain a part of a topic (a broker can contain M(N>M) partition)

### 删除 topic

```bat
bin\windows\kafka-topics.bat --zookeeper localhost:2181 --delete --topic my-replicated-topic
```

需要 server.properties 中设置 `delete.topic.enable=true` 否则只是标记删除。

### 查看某个 Topic 的详情

```bat
bin\windows\kafka-topics.bat --zookeeper localhost:2181 --describe --topic first
```

### 修改分区数

```bat
bin\windows\kafka-topics.bat --zookeeper localhost:2181 --alter --topic first --partitions 6
```

## 08.Kafka入门_命令行控制台生产者消费者测试

### 发送消息

```bat
bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic test

>hello, kafka.
>what a nice day!
>to be or not to be. that' s a question.
```

### 消费消息

```bat
bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test --from-beginning

hello, kafka.
what a nice day!
to be or not to be. that' s a question.
```

- --from-beginning： 会把主题中以往所有的数据都读取出来。

## 09.Kafka入门_数据日志分离

略

## 10.Kafka入门_回顾

略

## 11.Kafka高级_工作流程

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/05.png)

Kafka 中消息是以 topic 进行分类的， producer生产消息，consumer消费消息，都是面向 topic的。(从命令行操作看出)

```bat
bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic test

bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test --from-beginning
```

topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。（topic = N partition，partition = log）

Producer 生产的数据会被不断追加到该log 文件末端，且每条数据都有自己的 offset。 consumer组中的每个consumer， 都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费。（producer -> log with offset -> consumer(s)）

## 12.Kafka高级_文件存储

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/06.png)

由于生产者生产的消息会不断追加到 log 文件末尾， 为防止 log 文件过大导致数据定位效率低下， Kafka 采取了**分片**和**索引**机制，将每个 partition 分为多个 segment。

每个 segment对应两个文件——“.index”文件和“.log”文件。 这些文件位于一个文件夹下， 该文件夹的命名规则为： topic 名称+分区序号。例如， first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2。

```
00000000000000000000.index
00000000000000000000.log
00000000000000170410.index
00000000000000170410.log
00000000000000239430.index
00000000000000239430.log
```

index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log文件的结构示意图。

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/07.png)

**“.index”文件存储大量的索引信息，“.log”文件存储大量的数据**，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。

> segment 英 [ˈseɡmənt , seɡˈment] 美 [ˈseɡmənt , seɡˈment]
> n.部分;份;片;段;(柑橘、柠檬等的)瓣;弓形;圆缺 v.分割;划分

## 13.Kafka高级_生产者分区策略

### 分区的原因

1. **方便在集群中扩展**，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic又可以有多个 Partition 组成，因此整个集群就可以适应适合的数据了；
2. **可以提高并发**，因为可以以 Partition 为单位读写了。（联想到ConcurrentHashMap在高并发环境下读写效率比HashTable的高效）

### 分区的原则

我们需要将 producer 发送的数据封装成一个 `ProducerRecord` 对象。

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/08.png)

1. 指明 partition 的情况下，直接将指明的值直接作为 partiton 值；
2. 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；
3. 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition值，也就是常说的 round-robin 算法。轮询的方式进行分配

## 14.Kafka高级_生产者ISR

为保证 producer 发送的数据，能可靠的发送到指定的 topic， topic 的每个 partition 收到producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果producer 收到 ack， 就会进行下一轮的发送，否则重新发送数据。

> acknowledgement 英 [əkˈnɒlɪdʒmənt] 美 [əkˈnɑːlɪdʒmənt]
> n.(对事实、现实、存在的)承认;感谢;谢礼;**收件复函**

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/09.png)

**何时发送ack？**

确保有follower与leader同步完成，leader再发送ack，这样才能保证leader挂掉之后，能在follower中选举出新的leader。

------

**多少个follower同步完成之后发送ack？**

1. 半数以上的follower同步完成，即可发送ack继续发送重新发送
2. 全部的follower同步完成，才可以发送ack

### 副本数据同步策略

| 序号 | 方案                          | 优点                                                         | 缺点                                                         |
| ---- | ----------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | 半数以上完成同步， 就发送 ack | 延迟低                                                       | 选举新的 leader 时，容忍 n 台节点的故障，需要 2n+1 个副本。（如果集群有2n+1台机器，选举leader的时候至少需要半数以上即n+1台机器投票，那么能容忍的故障，最多就是n台机器发生故障）容错率：1/2 |
| 2    | 全部完成同步，才发送ack       | 选举新的 leader 时， 容忍 n 台节点的故障，需要 n+1 个副本（如果集群有n+1台机器，选举leader的时候只要有一个副本就可以了）容错率：1 | 延迟高                                                       |

Kafka 选择了第二种方案，原因如下：

1. 同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本，而 Kafka 的每个分区都有大量的数据， 第一种方案会造成大量数据的冗余。
2. 虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。

### ISR

采用第二种方案之后，设想以下情景： leader 收到数据，所有 follower 都开始同步数据，但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去，直到它完成同步，才能发送 ack。这个问题怎么解决呢？

Leader 维护了一个动态的 **in-sync replica set** (ISR)，意为和 leader 保持同步的 follower 集合。当 ISR 中的 follower 完成数据的同步之后，就会给 leader 发送 ack。如果 follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由`replica.lag.time.max.ms`参数设定。 Leader 发生故障之后，就会从 ISR 中选举新的 leader。

> **replica.lag.time.max.ms**
>
> **DESCRIPTION**: If a follower hasn't sent any fetch requests or hasn't consumed up to the leaders log end offset for at least this time, the leader will remove the follower from isr
>
> **TYPE**: long
>
> **DEFAULT**: 10000
>
> [Source](http://kafka.apache.org/0110/documentation/#brokerconfigs)

![image-20201110175537900](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/image-20201110175537900.png)

## 15.Kafka高级_生产者ACk机制

对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功。

所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。

**acks 参数配置**：

- 0： producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟， broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能**丢失数据**；
- 1： producer 等待 broker 的 ack， partition 的 leader 落盘成功后返回 ack，如果在 follower同步成功之前 leader 故障，那么将会**丢失数据**；

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/10.png)

- -1（all） ： producer 等待 broker 的 ack， partition 的 leader 和 ISR 的follower 全部落盘成功后才返回 ack。但是如果在 follower 同步完成后， broker 发送 ack 之前， leader 发生故障，那么会造成**数据重复**。

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/11.png)

**助记：返ACK前，0无落盘，1一落盘，-1全落盘，（落盘：消息存到本地）**

> **acks**
>
> **DESCRIPTION**:
>
> The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are allowed:
>
> - `acks=0` If set to zero then the producer will not wait for any acknowledgment from the server at all. The record will be immediately added to the socket buffer and considered sent. No guarantee can be made that the server has received the record in this case, and the retries configuration will not take effect (as the client won't generally know of any failures). The offset given back for each record will always be set to -1.
> - `acks=1` This will mean the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers. In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost.
> - `acks=all` This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee. This is equivalent to the acks=-1 setting.
>
> **TYPE**:string
>
> **DEFAULT**:1
>
> **VALID VALUES**:[all, -1, 0, 1]
>
> [Source](http://kafka.apache.org/0110/documentation/#producerconfigs)

## 16.Kafka高级_数据一致性问题

保证消费数据的一致性

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/12.png)

- LEO：（Log End Offset）每个副本的最后一个offset
- HW：（High Watermark）高水位，指的是消费者能见到的最大的 offset， ISR 队列中最小的 LEO

### follower 故障和 leader 故障

- **follower 故障**：follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后， follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。
- **leader 故障**：leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性， 其余的 follower 会先将各自的 log 文件**高于 HW 的部分截掉**，然后从新的 leader同步数据。

注意： 这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。

## 17.Kafka高级_ExactlyOnce



将服务器的 ACK 级别设置为-1（all），可以保证 Producer 到 Server 之间不会丢失数据，即 **At Least Once** 语义。

相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被发送一次，即 **At Most Once** 语义。

At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的， At Most Once可以保证数据不重复，但是不能保证数据不丢失。 但是，对于一些非常重要的信息，比如说**交易数据**，下游数据消费者要求数据既不重复也不丢失，即 **Exactly Once** 语义。

> - At least once—Messages are **never lost** but may be redelivered.
> - At most once—Messages **may be lost** but are never redelivered.
> - Exactly once—this is what people actually want, each message is delivered once and only once.
>
> [Source](http://kafka.apache.org/0110/documentation/#semantics)

在 0.11 版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。

0.11 版本的 Kafka，引入了一项重大特性：**幂等性**。**所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据， Server 端都只会持久化一条**。幂等性结合 At Least Once 语义，就构成了 Kafka 的 Exactly Once 语义。即：

```
At Least Once + 幂等性 = Exactly Once  
```

要启用幂等性，只需要将 Producer 的参数中 `enable.idempotence` 设置为 true 即可。 Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而Broker 端会对`<PID, Partition, SeqNumber>`做缓存，当具有相同主键的消息提交时， Broker 只会持久化一条。

但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。

**解决单个分区单次会话的数据一致性问题，producer重启以后就失效，因为会重新分配新的PID**

> **enable.idempotence**
>
> DESCRIPTION:When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. If 'false', producer retries due to broker failures, etc., may write duplicates of the retried message in the stream. This is set to 'false' by default. Note that enabling idempotence requires `max.in.flight.requests.per.connection` to be set to 1 and `retries` cannot be zero. Additionally acks must be set to 'all'. If these values are left at their defaults, we will override the default to be suitable. If the values are set to something incompatible with the idempotent producer, a ConfigException will be thrown.
>
> TYPE:boolean
>
> DEFAULT:false
>
> [Source](http://kafka.apache.org/0110/documentation/#producerconfigs)

## 18.Kafka高级_生产者总结

略

## 19.Kafka高级_消费者分区分配策略

### 消费方式

**consumer 采用 pull（拉） 模式从 broker 中读取数据**。

**push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的**。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。

**pull 模式不足之处**是，如果 kafka 没有数据，消费者可能会陷入循环中， 一直返回空数据。 针对这一点， Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有数据可供消费， consumer 会等待一段时间之后再返回，这段时长即为 timeout。

[Push vs. pull](http://kafka.apache.org/0110/documentation/#design_pull)



### 分区分配策略

**同一个组接收的主题是一样的。组的概念是同一个业务需求。**

**range和roundrobin的差别就是前者是各个topic独立划分消费partition，后者是将消费者组的所有topic的分区做成整体轮训划分给各个消费者**

一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。

Kafka 有两种分配策略：

- round-robin循环
- range  range是按照一个主题划分

> **partition.assignment.strategy**
>
> Select between the "range" or "roundrobin" strategy for assigning分配 partitions to consumer streams.
>
> The **round-robin** partition assignor lays out规划 all the available partitions and all the available consumer threads. It then proceeds to do接着做 a round-robin assignment from partition to consumer thread. If the subscriptions订阅 of all consumer instances are identical完全同样的, then the partitions will be uniformly 均匀地distributed. (i.e.也就是说, the partition ownership counts will be within a delta of exactly one across all consumer threads.) Round-robin assignment is permitted only if:
>
> 1. Every topic has the same number of streams within a consumer instance
> 2. The set of subscribed topics is identical for every consumer instance within the group.
>
> **Range** partitioning works on a per-**topic** basis. For each topic, we lay out the available partitions in numeric order and the consumer threads in lexicographic词典式的 order. We then divide the number of partitions by the total number of consumer streams (threads) to determine the number of partitions to assign to each consumer. If it does not evenly divide, then the first few consumers will have one extra partition.
>
> **DEFAULT**:range
>
> [Source](http://kafka.apache.org/0110/documentation/#oldconsumerconfigs)

------

[Kafka再平衡机制详解](https://zhuanlan.zhihu.com/p/86718818)

#### Round Robin

关于Roudn Robin重分配策略，其主要采用的是一种轮询的方式分配所有的分区，该策略主要实现的步骤如下。这里我们首先假设有三个topic：t0、t1和t2，这三个topic拥有的分区数分别为1、2和3，那么总共有六个分区，这六个分区分别为：t0-0、t1-0、t1-1、t2-0、t2-1和t2-2。这里假设我们有三个consumer：C0、C1和C2，它们订阅情况为：C0订阅t0，C1订阅t0和t1，C2订阅t0、t1和t2。那么这些分区的分配步骤如下：

- 首先将所有的partition和consumer按照字典序进行排序，所谓的字典序，就是按照其名称的字符串顺序，那么上面的六个分区和三个consumer排序之后分别为：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/16.png)

- 然后依次以按顺序轮询的方式将这六个分区分配给三个consumer，如果当前consumer没有订阅当前分区所在的topic，则轮询的判断下一个consumer：
- 尝试将t0-0分配给C0，由于C0订阅了t0，因而可以分配成功；
- 尝试将t1-0分配给C1，由于C1订阅了t1，因而可以分配成功；
- 尝试将t1-1分配给C2，由于C2订阅了t1，因而可以分配成功；
- 尝试将t2-0分配给C0，由于C0没有订阅t2，因而会轮询下一个consumer；
- 尝试将t2-0分配给C1，由于C1没有订阅t2，因而会轮询下一个consumer；
- 尝试将t2-0分配给C2，由于C2订阅了t2，因而可以分配成功；
- 同理由于t2-1和t2-2所在的topic都没有被C0和C1所订阅，因而都不会分配成功，最终都会分配给C2。
- 按照上述的步骤将所有的分区都分配完毕之后，最终分区的订阅情况如下：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/17.png)

从上面的步骤分析可以看出，轮询的策略就是简单的将所有的partition和consumer按照字典序进行排序之后，然后依次将partition分配给各个consumer，如果当前的consumer没有订阅当前的partition，那么就会轮询下一个consumer，直至最终将所有的分区都分配完毕。但是从上面的分配结果可以看出，轮询的方式会导致每个consumer所承载的分区数量不一致，从而导致各个consumer压力不均一。

#### Range

所谓的Range重分配策略，就是首先会计算各个consumer将会承载的分区数量，然后将指定数量的分区分配给该consumer。这里我们假设有两个consumer：C0和C1，两个topic：t0和t1，这两个topic分别都有三个分区，那么总共的分区有六个：t0-0、t0-1、t0-2、t1-0、t1-1和t1-2。那么Range分配策略将会按照如下步骤进行分区的分配：

- 需要注意的是，Range策略是按照topic依次进行分配的，比如我们以t0进行讲解，其首先会获取t0的所有分区：t0-0、t0-1和t0-2，以及所有订阅了该topic的consumer：C0和C1，并且会将这些分区和consumer按照字典序进行排序；
- 然后按照平均分配的方式计算每个consumer会得到多少个分区，如果没有除尽，则会将多出来的分区依次计算到前面几个consumer。比如这里是三个分区和两个consumer，那么每个consumer至少会得到1个分区，而3除以2后还余1，那么就会将多余的部分依次算到前面几个consumer，也就是这里的1会分配给第一个consumer，总结来说，那么C0将会从第0个分区开始，分配2个分区，而C1将会从第2个分区开始，分配1个分区；
- 同理，按照上面的步骤依次进行后面的topic的分配。
- 最终上面六个分区的分配情况如下：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/18.png)

可以看到，如果按照`Range`分区方式进行分配，其本质上是依次遍历每个topic，然后将这些topic的分区按照其所订阅的consumer数量进行平均的范围分配。这种方式从计算原理上就会导致排序在前面的consumer分配到更多的分区，从而导致各个consumer的压力不均衡。

TODO:我的问题：topic分多个partition，有些custom根据上述策略，分到topic的部分partition，难道不是要全部partition吗？是不是还要按照相同策略多分配多一次？

### 3. Sticky

`Sticky`策略是新版本中新增的策略，顾名思义，这种策略会保证再分配时已经分配过的分区尽量保证其能够继续由当前正在消费的consumer继续消费，当然，前提是每个consumer所分配的分区数量都大致相同，这样能够保证每个consumer消费压力比较均衡。关于这种分配方式的分配策略，我们分两种情况进行讲解，即初始状态的分配和某个consumer宕机时的分配情况。

### 3.1 初始分配

初始状态分配的特点是，所有的分区都还未分配到任意一个consumer上。这里我们假设有三个consumer：C0、C1和C2，三个topic：t0、t1和t2，这三个topic分别有1、2和3个分区，那么总共的分区为：t0-0、t1-0、t1-1、t2-0、t2-1和t2-2。关于订阅情况，这里C0订阅了t0，C1订阅了t0和1，C2则订阅了t0、t1和t2。这里的分区分配规则如下：

- 首先将所有的分区进行排序，排序方式为：首先按照当前分区所分配的consumer数量从低到高进行排序，如果consumer数量相同，则按照分区的字典序进行排序。这里六个分区由于所在的topic的订阅情况各不相同，因而其排序结果如下：



![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/v2-e239d825160aca3e29495483da3d8bac_1440w.jpg)



- 然后将所有的consumer进行排序，其排序方式为：首先按照当前consumer已经分配的分区数量有小到大排序，如果两个consumer分配的分区数量相同，则会按照其名称的字典序进行排序。由于初始时，这三个consumer都没有分配任何分区，因而其排序结果即为其按照字典序进行排序的结果：



![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/v2-546926059b2d43d408cd8d70b72acfe4_1440w.png)



- 然后将各个分区依次遍历分配给各个consumer，首先需要注意的是，这里的遍历并不是C0分配完了再分配给C1，而是每次分配分区的时候都整个的对所有的consumer从头开始遍历分配，如果当前consumer没有订阅当前分区，则会遍历下一个consumer。然后需要注意的是，在整个分配的过程中，各个consumer所分配的分区数是动态变化的，而这种变化是会体现在各个consumer的排序上的，比如初始时C0是排在第一个的，此时如果分配了一个分区给C0，那么C0就会排到最后，因为其拥有的分区数是最多的。上面的六个分区整体的分配流程如下：
- 首先将t2-0尝试分配给C0，由于C0没有订阅t2，因而分配不成功，继续轮询下一个consumer；
- 然后将t2-0尝试分配给C1，由于C1没有订阅t2，因而分配不成功，继续轮询下一个consumer；
- 接着将t2-0尝试分配给C2，由于C2订阅了t2，因而分配成功，此时由于C2分配的分区数发生变化，各个consumer变更后的排序结果为：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/v2-546926059b2d43d408cd8d70b72acfe4_1440w.png)



- 接下来的t2-1和t2-2，由于也只有C2订阅了t2，因而其最终还是会分配给C2，最终在t2-0、t2-1和t2-2分配完之后，各个consumer的排序以及其分区分配情况如下：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/v2-0965ceb2c1e10113f858c56b23563a1c_1440w.png)



- 接着继续分配t1-0，首先尝试将其分配给C0，由于C0没有订阅t1，因而分配不成功，继续轮询下一个consumer；
- 然后尝试将t1-0分配给C1，由于C1订阅了t1，因而分配成功，此时各个consumer以及其分配的分区情况如下：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/v2-eb1c23ad2f4d25fc1a9a5c3046a892d7_1440w.png)



- 同理，接下来会分配t1-1，虽然C1和C2都订阅了t1，但是由于C1排在C2前面，因而该分区会分配给C1，即：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/v2-f2298aff2fe5aab4479e3b2a1d659cf0_1440w.png)



- 最后，尝试将t0-0分配给C0，由于C0订阅了t0，因而分配成功，最终的分配结果为：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/v2-790c7480cfd03c357e306cc54e1ced40_1440w.png)



上面的分配过程中，需要始终注意的是，虽然示例中的consumer顺序始终没有变化，但这是由于各个分区分配之后正好每个consumer所分配的分区数量的排序结果与初始状态一致。这里读者也可以比较一下这种分配方式与前面讲解的`Round Robin`进行对比，可以很明显的发现，`Sticky`重分配策略分配得更加均匀一些。

## 20.Kafka高级_消费者offset的存储

consumer,topic,partition三元组，维护一个offset

由于 consumer 在消费过程中可能会出现断电宕机等故障， consumer 恢复后，需要从故障前的位置的继续消费，所以 **consumer 需要实时记录自己消费到了哪个 offset**，以便故障恢复后继续消费。

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/13.png)

**Kafka 0.9 版本之前， consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始，consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets**。

1. 修改配置文件 consumer.properties，`exclude.internal.topics=false`。
2. 读取 offset
   - 0.11.0.0 之前版本 - `bin/kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter "kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter" --consumer.config config/consumer.properties --from-beginning`
   - 0.11.0.0 及之后版本 - `bin/kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter" --consumer.config config/consumer.properties --from-beginning`

消费者的消费记录等信息放到kakka的一个队列中了

## 21.Kafka高级_消费者组案例

他现在想要测试的是没加入一个consumer都会随机分配分区给消费者组里面的消费者，你们用py看视频吗

### 需求

测试同一个消费者组中的消费者， **同一时刻只能有一个**消费者消费。

### 操作步骤

1.修改`%KAFKA_HOME\config\consumer.properties%`文件中的`group.id`属性。

```properties
group.id=shan-kou-zu
```

2.打开两个cmd，分别启动两个消费者。（以`%KAFKA_HOME\config\consumer.properties%`作配置参数）

```bat
bin\windows\kafka-console-consumer.bat --zookeeper 127.0.0.1:2181 --topic test --consumer.config config\consumer.properties
```

3.再打开一个cmd，启动一个生产者。

```bat
bin\windows\kafka-console-producer.bat --broker-list 127.0.0.1:9092 --topic test
```

4.在生产者窗口输入消息，观察两个消费者窗口。**会发现两个消费者窗口中，只有一个才会弹出消息**。

## 22.Kafka高级_高效读写&ZK作用

### 1.顺序写磁盘

Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。 官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其**省去了大量磁头寻址的时间**。

### 2.零复制技术

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/14.png)

- NIC network interface controller 网络接口控制器

kafka用的是sendfile技术

### Zookeeper 在 Kafka 中的作用

Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所有 topic 的分区副本分配和 leader 选举等工作。[Reference](http://kafka.apache.org/0110/documentation/#design_replicamanagment)

Controller 的管理工作都是依赖于 Zookeeper 的。

以下为 partition 的 leader 选举过程：

![Leader选举流程](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/15.png)

## 23.Kafka高级_Ranger分区再分析

略

## 24.Kafka高级_事务

Kafka 从 0.11 版本开始引入了事务支持。事务可以保证 Kafka 在 Exactly Once 语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。

### Producer 事务

为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer 获得的PID 和Transaction ID 绑定。这样当Producer 重启后就可以通过正在进行的 TransactionID 获得原来的 PID。

为了管理 Transaction， Kafka 引入了一个新的组件 Transaction Coordinator。 Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。 Transaction Coordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。

### Consumer 事务

上述事务机制主要是从 Producer 方面考虑，对于 Consumer 而言，事务的保证就会相对较弱，尤其时无法保证 Commit 的信息被精确消费。这是由于 Consumer 可以通过 offset 访问任意信息，而且不同的 Segment File 生命周期不同，同一事务的消息可能会出现重启后被删除的情况。

## 25.Kafka高级_API生产者流程

Kafka 的 Producer 发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main 线程和 Sender 线程，以及一个线程共享变量——RecordAccumulator。 main 线程将消息发送给 RecordAccumulator， Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka broker。

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/19.png)

相关参数：

- **batch.size**： 只有数据积累到 batch.size 之后， sender 才会发送数据。
- **linger.ms**： 如果数据迟迟未达到 batch.size， sender 等待 linger.time 之后就会发送数据。

## 26.Kafka高级_异步发送API普通生产者

### 导入依赖

[pom.xml](https://my.oschina.net/jallenkwong/blog/pom.xml)

```xml
<dependency>
	<groupId>org.apache.kafka</groupId>
	<artifactId>kafka-clients</artifactId>
	<version>0.11.0.0</version>
</dependency>
```

### 编写代码

需要用到的类：

- KafkaProducer：需要创建一个生产者对象，用来发送数据
- ProducerConfig：获取所需的一系列配置参数
- ProducerRecord：每条数据都要封装成一个 ProducerRecord 对象

[CustomProducer.java](https://gitee.com/jallenkwong/LearnKafka/blob/master/src/main/java/com/lun/kafka/producer/CustomProducer.java)

```java
import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;

public class CustomProducer {

	public static void main(String[] args) {
		Properties props = new Properties();
		// kafka 集群， broker-list
		
		props.put("bootstrap.servers", "127.0.0.1:9092");
		
		//可用ProducerConfig.ACKS_CONFIG 代替 "acks"
		//props.put(ProducerConfig.ACKS_CONFIG, "all");
		props.put("acks", "all");
		// 重试次数
		props.put("retries", 1);
		// 批次大小
		props.put("batch.size", 16384);
		// 等待时间
		props.put("linger.ms", 1);
		// RecordAccumulator 缓冲区大小
		props.put("buffer.memory", 33554432);
		props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		Producer<String, String> producer = new KafkaProducer<>(props);
		for (int i = 0; i < 100; i++) {
			producer.send(new ProducerRecord<String, String>("test", "test-" + Integer.toString(i),
					"test-" + Integer.toString(i)));
		}
		producer.close();
	}

}
```

## 28.Kafka案例_异步发送API带回调函数的生产者

回调函数会在 producer 收到 ack 时调用，为异步调用， 该方法有两个参数，分别是 RecordMetadata 和 Exception，如果 Exception 为 null，说明消息发送成功，如果Exception 不为 null，说明消息发送失败。

**注意**：消息发送失败会自动重试，不需要我们在回调函数中手动重试。

[CallBackProducer.java](https://gitee.com/jallenkwong/LearnKafka/blob/master/src/main/java/com/lun/kafka/producer/CallBackProducer.java)

```java
import java.util.Properties;

import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

public class CallBackProducer {
	public static void main(String[] args) {
		Properties props = new Properties();
		props.put("bootstrap.servers", "127.0.0.1:9092");//kafka 集群， broker-list
		props.put("acks", "all");
		props.put("retries", 1);//重试次数
		props.put("batch.size", 16384);//批次大小
		props.put("linger.ms", 1);//等待时间
		props.put("buffer.memory", 33554432);//RecordAccumulator 缓冲区大小
		props.put("key.serializer",
		"org.apache.kafka.common.serialization.StringSerializer");
		props.put("value.serializer",
		"org.apache.kafka.common.serialization.StringSerializer");
		
		Producer<String, String> producer = new KafkaProducer<>(props);
		for (int i = 0; i < 100; i++) {
			producer.send(new ProducerRecord<String, String>("test",
				"test" + Integer.toString(i)), new Callback() {
			
				//回调函数， 该方法会在 Producer 收到 ack 时调用，为异步调用
				@Override
				public void onCompletion(RecordMetadata metadata, Exception exception) {
					if (exception == null) {
						System.out.println(metadata.partition() + " - " + metadata.offset());
					} else {
						exception.printStackTrace();
					}
				}
			});
		}
		
		producer.close();
	}
}
```

## 29.Kafka案例_API生产者分区策略测试

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20.png)

ProducerRecord类有许多构造函数，其中一个参数partition可指定分区

TODO:根据策略阐述，亲自设计分区策略测试，range和round-robin间的区别

## 30.Kafka案例_API带自定义分区器的生成者

[MyPartitioner.java](https://gitee.com/jallenkwong/LearnKafka/blob/master/src/main/java/com/lun/kafka/producer/MyPartitioner.java)

```java
import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;

public class MyPartitioner implements Partitioner {

	@Override
	public void configure(Map<String, ?> configs) {
		// TODO Auto-generated method stub

	}

	@Override
	public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public void close() {
		// TODO Auto-generated method stub

	}

}
```

具体内容填写可参考默认分区器`org.apache.kafka.clients.producer.internals.DefaultPartitioner`

然后Producer配置中注册使用

```java
Properties props = new Properties();
...
props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class);
...
Producer<String, String> producer = new KafkaProducer<>(props);
```

## 31.Kafka案例_API同步发送生成者

同步发送的意思就是，一条消息发送之后，会阻塞当前线程， 直至返回 ack。

由于 send 方法返回的是一个 Future 对象，根据 Futrue 对象的特点，我们也可以实现同步发送的效果，只需在调用 Future 对象的 get 方发即可。

SyncProducer.java

```java
Producer<String, String> producer = new KafkaProducer<>(props);
for (int i = 0; i < 100; i++) {
	producer.send(new ProducerRecord<String, String>("test",  "test - 1"), new Callback() {
		@Override
		public void onCompletion(RecordMetadata metadata, Exception exception) {
			...
		}
	}).get();//<----------------------
}
```

## 32.Kafka案例_API简单消费者

- **KafkaConsumer**： 需要创建一个消费者对象，用来消费数据
- **ConsumerConfig**： 获取所需的一系列配置参数
- **ConsuemrRecord**： 每条数据都要封装成一个 ConsumerRecord 对象

**为了使我们能够专注于自己的业务逻辑， Kafka 提供了自动提交 offset 的功能**。

自动提交 offset 的相关参数：

- **enable.auto.commit**：是否开启自动提交 offset 功能
- **auto.commit.interval.ms**：自动提交 offset 的时间间隔

[CustomConsumer.java](https://gitee.com/jallenkwong/LearnKafka/blob/master/src/main/java/com/lun/kafka/consumer/CustomConsumer.java)

```
import java.util.Arrays;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

public class CustomConsumer {
	public static void main(String[] args) {
		
		Properties props = new Properties();
		
		props.put("bootstrap.servers", "127.0.0.1:9092");
		props.put("group.id", "abc");
		props.put("enable.auto.commit", "true");
		props.put("auto.commit.interval.ms", "1000");
		props.put("key.deserializer",
				"org.apache.kafka.common.serialization.StringDeserializer");
		props.put("value.deserializer",
				"org.apache.kafka.common.serialization.StringDeserializer");
		
		KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
		
		consumer.subscribe(Arrays.asList("test"));
		
		while (true) {
			ConsumerRecords<String, String> records = consumer.poll(100);
			for (ConsumerRecord<String, String> record : records) {
				System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
			}
		}
	}
}
```

## 33.Kafka案例_API消费者重置offset

Consumer 消费数据时的可靠性是很容易保证的，因为数据在 Kafka 中是持久化的，故不用担心数据丢失问题。

由于 consumer 在消费过程中可能会出现断电宕机等故障， consumer 恢复后，需要从故障前的位置的继续消费，所以** consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费**。

**所以 offset 的维护是 Consumer 消费数据是必须考虑的问题**。

```java
public static final String AUTO_OFFSET_RESET_CONFIG = "auto.offset.reset";
Properties props = new Properties();
...
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
props.put("group.id", "abcd");//组id需另设，否则看不出上面一句的配置效果
...
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
```

从结果看，`props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");`与命令行中`bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test --from-beginning`的`--from-beginning`拥有相同的作用。

------

> **--from-beginning**
>
> If the consumer does not already have an established offset to consume from, start with the earliest message present in the log rather than the latest message.
>
> From [kafka-console-sonsumer.bat](https://my.oschina.net/jallenkwong/blog/4449224)

------

> **auto.offset.reset**
>
> What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted):
>
> - **earliest**: automatically reset the offset to the earliest offset
> - **latest**: automatically reset the offset to the latest offset
> - **none**: throw exception to the consumer if no previous offset is found for the consumer's group
> - **anything else**: throw exception to the consumer.
>
> **TYPE**:string
>
> **DEFAULT**:latest
>
> **VALID VALUES**:[latest, earliest, none]
>
> [Source](http://kafka.apache.org/0110/documentation/#newconsumerconfigs)

## 34.Kafka案例_消费者保存offset读取问题

消费者保存offset的消费信息在自己的内存里，只有在启动的时候才会去kafka服务器拉取一次，然后就将消费的offset信息保存在本地，而不是每次都去服务器拉取

```java
props.put("enable.auto.commit", "true");
```

> **enable.auto.commit**
>
> If true the consumer's offset will be periodically committed in the background.
>
> **TYPE**:boolean
>
> **DEFAULT**:true
>
> [Source](http://kafka.apache.org/0110/documentation/#newconsumerconfigs)

PS.我将Offset提交类比成数据库事务的提交。

## 35.Kafka案例_API消费者手动提交offset

虽然自动提交 offset 十分便利，但由于其是基于时间提交的， 开发人员难以把握offset 提交的时机。因此 **Kafka 还提供了手动提交 offset 的 API**。

手动提交 offset 的方法有两种：

1. commitSync（同步提交）
2. commitAsync（异步提交）

两者的**相同点**是，都会将本次 poll 的一批数据最高的偏移量提交；

**不同点**是，commitSync 阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而 commitAsync 则没有失败重试机制，故有可能提交失败。

### 同步提交offset

由于同步提交 offset 有失败重试机制，故更加可靠，以下为同步提交 offset 的示例。

 SyncCommitOffset.java 

```java
public class SyncCommitOffset {
	public static void main(String[] args) {
		Properties props = new Properties();
		//Kafka 集群
		props.put("bootstrap.servers", "hadoop102:9092");
		//消费者组，只要 group.id 相同，就属于同一个消费者组
		props.put("group.id", "test");
		props.put("enable.auto.commit", "false");//关闭自动提交 offset
		props.put("key.deserializer",
				"org.apache.kafka.common.serialization.StringDeserializer");
		props.put("value.deserializer",
				"org.apache.kafka.common.serialization.StringDeserializer");
		KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
		consumer.subscribe(Arrays.asList("first"));//消费者订阅主题
		while (true) {
			//消费者拉取数据
			ConsumerRecords<String, String> records =
			consumer.poll(100);
			for (ConsumerRecord<String, String> record : records) {
				System.out.printf("offset = %d, key = %s, value= %s%n", record.offset(), record.key(), record.value());
			}
			//同步提交，当前线程会阻塞直到 offset 提交成功
			consumer.commitSync();
		}
	}
}
```

### 异步提交offset

虽然同步提交 offset 更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交 offset 的方式。

[AsyncCommitOffset.java]()

```java
public class AsyncCommitOffset {
	public static void main(String[] args) {
		Properties props = new Properties();
		...
		//<--------------------------------------
		//关闭自动提交
		props.put("enable.auto.commit", "false");
		...
		KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
		consumer.subscribe(Arrays.asList("first"));// 消费者订阅主题
		while (true) {
			ConsumerRecords<String, String> records = consumer.poll(100);// 消费者拉取数据
			for (ConsumerRecord<String, String> record : records) {
				System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
			}
			//<----------------------------------------------
			// 异步提交
			consumer.commitAsync(new OffsetCommitCallback() {
				@Override
				public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {
					if (exception != null) {
						System.err.println("Commit failed for" + offsets);
					}
				}
			});
		}
	}
}
```

### 数据漏消费和重复消费分析

无论是同步提交还是异步提交 offset，都有可能会造成数据的漏消费或者重复消费。先提交 offset 后消费，有可能造成数据的漏消费；而先消费后提交 offset，有可能会造成数据的重复消费。

### 自定义存储 offset

Kafka 0.9 版本之前， offset 存储在 zookeeper， 0.9 版本及之后，默认将 offset 存储在 Kafka的一个内置的 topic 中。除此之外， Kafka 还可以选择自定义存储 offset。

offset 的维护是相当繁琐的， 因为需要考虑到消费者的 rebalance。

**当有新的消费者加入消费者组、 已有的消费者推出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做 Rebalance**。

消费者发生 Rebalance 之后，每个消费者消费的分区就会发生变化。**因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的 offset 位置继续消费**。

要实现自定义存储 offset，需要借助 `ConsumerRebalanceListener`， 以下为示例代码，其中提交和获取 offset 的方法，需要根据所选的 offset 存储系统自行实现。(可将offset存入MySQL数据库)

[CustomSaveOffset.java](https://gitee.com/jallenkwong/LearnKafka/blob/master/src/main/java/com/lun/kafka/consumer/CustomSaveOffset.java)

```java
public class CustomSaveOffset {
	private static Map<TopicPartition, Long> currentOffset = new HashMap<>();

	public static void main(String[] args) {
 public static void main(String[] args) {
        // 创建配置信息
        Properties props = new Properties();
        // Kafka 集群
        props.put("bootstrap.servers", "hadoop102:9092");
        // 消费者组，只要 group.id 相同，就属于同一个消费者组
        props.put("group.id", "test");
        // 关闭自动提交 offset
        props.put("enable.auto.commit", "false");
        // Key 和 Value 的反序列化类
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        // 创建一个消费者
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        // 消费者订阅主题
        consumer.subscribe(Arrays.asList("first"), new ConsumerRebalanceListener() {
            // 该方法会在 Rebalance 之前调用
            @Override
            public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
                commitOffset(currentOffset);
            }

            // 该方法会在 Rebalance 之后调用
            @Override
            public void onPartitionsAssigned(Collection<TopicPartition> partitions) {

                currentOffset.clear();
                for (TopicPartition partition : partitions) {
                    consumer.seek(partition, getOffset(partition));// 定位到最近提交的 offset 位置继续消费
                }
            }
        });

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(100);// 消费者拉取数据
            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
                currentOffset.put(new TopicPartition(record.topic(), record.partition()), record.offset());
            }
            commitOffset(currentOffset);// 异步提交
        }
    }

    // 获取某分区的最新 offset
    private static long getOffset(TopicPartition partition) {
        return 0;
    }

    // 提交该消费者所有分区的 offset
    private static void commitOffset(Map<TopicPartition, Long> currentOffset) {
    }
}
```

## 36.Kafka案例_API自定义拦截器（需求分析）

### 拦截器原理

Producer 拦截器(interceptor)是在 Kafka 0.10 版本被引入的，主要用于实现 clients 端的定制化控制逻辑。

对于 producer 而言， interceptor 使得用户在消息发送前以及 producer 回调逻辑前有机会对消息做一些定制化需求，比如`修改消息`等。同时， producer 允许用户指定多个 interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。 Intercetpor 的实现接口是`org.apache.kafka.clients.producer.ProducerInterceptor`，其定义的方法包括：

- `configure(configs)`：获取配置信息和初始化数据时调用。
- `onSend(ProducerRecord)`：该方法封装进 KafkaProducer.send 方法中，即它运行在用户主线程中。 Producer 确保**在消息被序列化以及计算分区前**调用该方法。 用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的 topic 和分区， 否则会影响目标分区的计算。
- `onAcknowledgement(RecordMetadata, Exception)`：**该方法会在消息从 RecordAccumulator 成功发送到 Kafka Broker 之后，或者在发送过程中失败时调用**。 并且通常都是在 producer 回调逻辑触发之前。 onAcknowledgement 运行在producer 的 IO 线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢 producer 的消息发送效率。
- `close()`：关闭 interceptor，主要用于执行一些**资源清理**工作

如前所述， interceptor 可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外**倘若指定了多个 interceptor，则 producer 将按照指定顺序调用它们**，并仅仅是捕获每个 interceptor 可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。

## 37.Kafka案例_API自定义拦截器（代码实现）

### 需求

实现一个简单的双 interceptor 组成的拦截链。

- 第一个 interceptor 会在消息发送前将时间戳信息加到消息 value 的最前部；
- 第二个 interceptor 会在消息发送后更新成功发送消息数或失败发送消息数。

### 案例实操

#### 增加时间戳拦截器

[TimeInterceptor.java](https://gitee.com/jallenkwong/LearnKafka/blob/master/src/main/java/com/lun/kafka/interceptor/TimeInterceptor.java)

```java
import java.util.Map;

import org.apache.kafka.clients.producer.ProducerInterceptor;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

public class TimeInterceptor implements ProducerInterceptor<String, String> {
	@Override
	public void configure(Map<String, ?> configs) {
	}

	@Override
	public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
		// 创建一个新的 record，把时间戳写入消息体的最前部
		return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(),
				"TimeInterceptor: " + System.currentTimeMillis() + "," + record.value().toString());
	}

	@Override
	public void close() {
	}

	@Override
	public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
		// TODO Auto-generated method stub
		
	}
}
```

#### 增加时间戳拦截器

统计发送消息成功和发送失败消息数，并在 producer 关闭时打印这两个计数器

[CounterInterceptor.java](https://gitee.com/jallenkwong/LearnKafka/blob/master/src/main/java/com/lun/kafka/interceptor/CounterInterceptor.java)

```java
import java.util.Map;

import org.apache.kafka.clients.producer.ProducerInterceptor;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

public class CounterInterceptor implements ProducerInterceptor<String, String>{

	private int errorCounter = 0;
	private int successCounter = 0;
	
	@Override
	public void configure(Map<String, ?> configs) {
		// TODO Auto-generated method stub
		
	}

	@Override
	public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
		return record;
	}

	@Override
	public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
		// 统计成功和失败的次数
		if (exception == null) {
			successCounter++;
		} else {
			errorCounter++;
		}
	}

	@Override
	public void close() {
		// 保存结果
		System.out.println("Successful sent: " + successCounter);
		System.out.println("Failed sent: " + errorCounter);
		
	}

}
```

#### producer 主程序

[InterceptorProducer.java](https://gitee.com/jallenkwong/LearnKafka/blob/master/src/main/java/com/lun/kafka/producer/InterceptorProducer.java)

```java
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;

public class InterceptorProducer {
	public static void main(String[] args) {
		// 1 设置配置信息
		Properties props = new Properties();
		props.put("bootstrap.servers", "127.0.0.1:9092");
		props.put("acks", "all");
		props.put("retries", 3);
		props.put("batch.size", 16384);
		props.put("linger.ms", 1);
		props.put("buffer.memory", 33554432);
		props.put("key.serializer",
				"org.apache.kafka.common.serialization.StringSerializer");
		props.put("value.serializer",
				"org.apache.kafka.common.serialization.StringSerializer");
		//<--------------------------------------------
		// 2 构建拦截链
		List<String> interceptors = new ArrayList<>();
		interceptors.add("com.lun.kafka.interceptor.TimeInterceptor");
		interceptors.add("com.lun.kafka.interceptor.CounterInterceptor");
		
		props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);
		
		String topic = "test";
		Producer<String, String> producer = new KafkaProducer<>(props);
		// 3 发送消息
		for (int i = 0; i < 10; i++) {
			ProducerRecord<String, String> record = new ProducerRecord<>(topic, "message" + i);
			producer.send(record);
		}
		
		// 4 一定要关闭 producer，这样才会调用 interceptor 的 close 方法
		producer.close();
		
	}
}
```

## 38.Kafka案例_API自定义拦截器（案例测试）

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/21.png)

## 39.Kafka案例_监控Eagle的安装

- [Kafka Eagle官方主页](https://www.kafka-eagle.org/)
- [Kafka Eagle下载页面](http://download.kafka-eagle.org/)
- [Kafka Eagle官方文档](https://www.kafka-eagle.org/articles/docs/documentation.html)

### 什么是Kafka Eagle

> Kafka Eagle is open source visualization and management software. It allows you to query, visualize, alert on, and explore your metrics no matter where they are stored. In plain English, it provides you with tools to turn your kafka cluster data into beautiful graphs and visualizations.
>
> Kafka Eagle是开源可视化和管理软件。它允许您查询、可视化、提醒和探索您的指标，无论它们存储在哪里。简单地说，它为您提供了将kafka集群数据转换为漂亮的图形和可视化的工具。
>
> [Source](https://www.kafka-eagle.org/articles/docs/introduce/what-is-kafka-eagle.html)

一个运行在Tomcat的Web应用。

### 安装与运行

#### 在Linux上安装与运行

略

#### 在Windows上安装与运行

- 安装JDK，设置环境变量时，**路径最好不要带空格**，否则，后序运行ke.bat抛异常。若路径必须带有空格，可以通过小技巧，让ke.bat成功运行。这个技巧是：若你的`JAVA_HOME`的变量值为`C:\Program Files\Java\jdk1.8.0_161`，则将其改成`C:\progra~1\Java\jdk1.8.0_161`。
- 到[Kafka Eagle下载页面](http://download.kafka-eagle.org/)下载安装包。也可在网盘下载，链接在[本文首部](https://my.oschina.net/jallenkwong/blog/4449224#)
- 解压安装包，然后设置环境变量`KE_HOME`，其值如`C:\Kafka\kafka-eagle-web-1.3.7`。若想打开cmd输入命令`ke.bat`运行Kafka Eagle，在`PATH`环境变量的值头添加`%KE_HOME%\bin;`
- 修改配置文件`%KE_HOME%\conf\system-config.properties`
- (可选)Kafka Server的JVM调参，用文本编辑器打开`%KAFKA_HOME%\bin\windows\kafka-server-start.bat`，其中的`set KAFKA_HEAP_OPTS=-Xmx1G -Xms1G`改为`set KAFKA_HEAP_OPTS=-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70`

```properties
######################################
# multi zookeeper&kafka cluster list
######################################<---设置ZooKeeper的IP地址
kafka.eagle.zk.cluster.alias=cluster1
cluster1.zk.list=localhost:2181

######################################
# zk client thread limit
######################################
kafka.zk.limit.size=25

######################################
# kafka eagle webui port
######################################
kafka.eagle.webui.port=8048

######################################
# kafka offset storage
######################################
cluster1.kafka.eagle.offset.storage=kafka
#cluster2.kafka.eagle.offset.storage=zk

######################################
# enable kafka metrics
######################################<---metrics.charts从false改成true
kafka.eagle.metrics.charts=true
kafka.eagle.sql.fix.error=false

######################################
# kafka sql topic records max
######################################
kafka.eagle.sql.topic.records.max=5000

######################################
# alarm email configure
######################################
kafka.eagle.mail.enable=false
kafka.eagle.mail.sa=alert_sa@163.com
kafka.eagle.mail.username=alert_sa@163.com
kafka.eagle.mail.password=mqslimczkdqabbbh
kafka.eagle.mail.server.host=smtp.163.com
kafka.eagle.mail.server.port=25

######################################
# alarm im configure
######################################
#kafka.eagle.im.dingding.enable=true
#kafka.eagle.im.dingding.url=https://oapi.dingtalk.com/robot/send?access_token=

#kafka.eagle.im.wechat.enable=true
#kafka.eagle.im.wechat.token=https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=xxx&corpsecret=xxx
#kafka.eagle.im.wechat.url=https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=
#kafka.eagle.im.wechat.touser=
#kafka.eagle.im.wechat.toparty=
#kafka.eagle.im.wechat.totag=
#kafka.eagle.im.wechat.agentid=

######################################
# delete kafka topic token
######################################
kafka.eagle.topic.token=keadmin

######################################
# kafka sasl authenticate
######################################
cluster1.kafka.eagle.sasl.enable=false
cluster1.kafka.eagle.sasl.protocol=SASL_PLAINTEXT
cluster1.kafka.eagle.sasl.mechanism=PLAIN
cluster1.kafka.eagle.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="kafka-eagle";

#cluster2 在此没有用到，将其注释掉
#cluster2.kafka.eagle.sasl.enable=false
#cluster2.kafka.eagle.sasl.protocol=SASL_PLAINTEXT
#cluster2.kafka.eagle.sasl.mechanism=PLAIN
#cluster2.kafka.eagle.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="kafka-eagle";

######################################
# kafka jdbc driver address
######################################
kafka.eagle.driver=org.sqlite.JDBC
#将url设置在本地
kafka.eagle.url=jdbc:sqlite:/C:/Kafka/kafka-eagle-web-1.3.7/db/ke.db
#进入系统需要用到的账号与密码
kafka.eagle.username=root
kafka.eagle.password=123456
```

- 运行
  - 运行ZooKeeper
  - 运行Kafka集群，另外运行kafka server前，需设置JMX_PORT，否则Kafka Eagle 后台提示连接失败。执行命令行`set JMX_PORT=9999 & start bin\windows\kafka-server-start.bat config\server.properties`设置JMX_PORT且运行Kafkaserver。在单节点开启Kafka集群，小心端口号冲突。
  - 点击`%KE_HOME%\bin\ke.bat`，运行Kafka Eagle。
  - 打开浏览器，在地址栏输入`http://localhost:8048/ke/`，然后在登录页面，输入在配置文件`%KE_HOME%\conf\system-config.properties`设置的账号与密码。
  - 登录成功，便可进入Kafka Eagle

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/22.png)

## 40.Kafka案例_监控Eagle的使用

[QUICK START](https://www.kafka-eagle.org/articles/docs/quickstart/dashboard.html)

## 41.Kafka案例_Kafka之与Flume对接

> Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量**日志采集、聚合和传输的系统**，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。
>
> [Source](https://baike.baidu.com/item/flume/6250098)

TODO:学习Flume时，再补充

## 42.Kafk之与Flume对接（数据分类）

TODO:学习Flume时，再补充

# 43.Kafka之Kafka面试题

1. Kafka 中是怎么体现消息顺序性的？
2. Kafka 中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？
3. Kafka 生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？
4. “消费组中的消费者个数如果超过 topic 的分区，那么就会有消费者消费不到数据”这句 话是否正确？
5. 消费者提交消费位移时提交的是当前消费到的最新消息的 offset 还是 offset+1？
   **offset+1**
6. 有哪些情形会造成重复消费？
   
7. 那些情景会造成消息漏消费？
   
8. 当你使用 kafka-topics.sh 创建（删除）了一个 topic 之后， Kafka 背后会执行什么逻辑？
   - 会在 zookeeper 中的/brokers/topics 节点下创建一个新的 topic 节点，如：/brokers/topics/first
   - 触发 Controller 的监听程序
   - kafka Controller 负责 topic 的创建工作，并更新 metadata cache
9. topic 的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？
   可增加，不可减少
10. topic 的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？
11. Kafka 有内部的 topic 吗？如果有是什么？有什么所用？
    有，存储以消费者消费的offset用的
12. Kafka 分区分配的概念？
    高版本又增加了stick方式
13. 简述 Kafka 的日志目录结构？
14. 如果我指定了一个 offset， Kafka Controller 怎么查找到对应的消息？
    二分查找 根据开始偏移量和长度
15. 聊一聊 Kafka Controller 的作用？
    
16. Kafka 中有那些地方需要选举？这些地方的选举策略又有哪些？
    controller  抢资源
    leader  ISR 同步的时间和条数（0.9干掉了）
17. 失效副本是指什么？有那些应对措施？
     
18. Kafka 的哪些设计让它有如此高的性能？
    分布式 顺序写 o0拷贝

![image-20201111210246321](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/image-20201111210246321.png)

[Kafka常见面试题(附个人解读答案+持续更新)](https://blog.csdn.net/C_Xiang_Falcon/article/details/100917145)

## 题库

### 1.Kafka中的ISR(InSyncRepli)、OSR(OutSyncRepli)、AR(AllRepli)代表什么？

ISR : 速率和leader相差低于10秒的follower的集合
OSR : 速率和leader相差大于10秒的follower
AR : 所有分区的follower

## 2.Kafka中的HW、LEO等分别代表什么？

HW : 又名高水位,根据同一分区中,最低的LEO所决定
LEO : 每个分区的最高offset

## 3.Kafka的用途有哪些？使用场景如何？

1.用户追踪:根据用户在web或者app上的操作,将这些操作消息记录到各个topic中,然后消费者通过订阅这些消息做实时的分析,或者记录到HDFS,用于离线分析或数据挖掘
2.日志收集:通过kafka对各个服务的日志进行收集,再开放给各个consumer
3.消息系统:缓存消息
4.运营指标:记录运营监控数据,收集操作应用数据的集中反馈,如报错和报告

## 4.Kafka中是怎么体现消息顺序性的？

每个分区内,每条消息都有offset,所以只能在同一分区内有序,但不同的分区无法做到消息顺序性

## 5.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确?

对的,超过分区数的消费者就不会再接收数据

## 6. 有哪些情形会造成重复消费？或丢失信息?

先处理后提交offset,会造成重读消费
先提交offset后处理,会造成数据丢失

## 7.Kafka 分区的目的？

对于kafka集群来说,分区可以做到负载均衡,对于消费者来说,可以提高并发度,提高读取效率

## 8.Kafka 的高可靠性是怎么实现的?

为了实现高可靠性,kafka使用了订阅的模式,并使用isr和ack应答机制
能进入isr中的follower和leader之间的速率不会相差10秒
当ack=0时,producer不等待broker的ack,不管数据有没有写入成功,都不再重复发该数据
当ack=1时,broker会等到leader写完数据后,就会向producer发送ack,但不会等follower同步数据,如果这时leader挂掉,producer会对新的leader发送新的数据,在old的leader中不同步的数据就会丢失
当ack=-1或者all时,broker会等到leader和isr中的所有follower都同步完数据,再向producer发送ack,有可能造成数据重复

## 9.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？

可以增加

```
bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-config --partitions 3
1
```

## 10.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？

不可以,先有的分区数据难以处理

## 11.简述Kafka的日志目录结构？

每一个分区对应一个文件夹,命名为topic-0,topic-1,每个文件夹内有.index和.log文件

## 12.如何解决消费者速率低的问题?

增加分区数和消费者数

## 13.Kafka的那些设计让它有如此高的性能？?

1.kafka是分布式的消息队列
2.对log文件进行了segment,并对segment建立了索引
3.(对于单节点)使用了顺序读写,速度可以达到600M/s
4.引用了zero拷贝,在os系统就完成了读写操作

## 14.kafka启动不起来的原因?

在关闭kafka时,先关了zookeeper,就会导致kafka下一次启动时,会报节点已存在的错误
只要把zookeeper中的zkdata/version-2的文件夹删除即可

## 15.聊一聊Kafka Controller的作用？

负责kafka集群的上下线工作,所有topic的副本分区分配和选举leader工作

## 16.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？

在ISR中需要选择,选择策略为先到先得

## 17.失效副本是指什么？有那些应对措施？

失效副本为速率比leader相差大于10秒的follower
将失效的follower先提出ISR
等速率接近leader10秒内,再加进ISR

## 18.Kafka消息是采用Pull模式，还是Push模式？

在producer阶段,是向broker用Push模式
在consumer阶段,是向broker用Pull模式
在Pull模式下,consumer可以根据自身速率选择如何拉取数据,避免了低速率的consumer发生崩溃的问题
但缺点是,consumer要时不时的去询问broker是否有新数据,容易发生死循环,内存溢出

## 19.Kafka创建Topic时如何将分区放置到不同的Broker中?

首先副本数不能超过broker数
第一分区是随机从Broker中选择一个,然后其他分区相对于0号分区依次向后移
第一个分区是从nextReplicaShift决定的,而这个数也是随机产生的

## 20.Kafka中的事务是怎么实现的?☆☆☆☆☆

kafka事务有两种
producer事务和consumer事务
producer事务是为了解决kafka跨分区跨会话问题
kafka不能跨分区跨会话的主要问题是每次启动的producer的PID都是系统随机给的
所以为了解决这个问题
我们就要手动给producer一个全局唯一的id,也就是transaction id 简称TID
我们将TID和PID进行绑定,在producer带着TID和PID第一次向broker注册时,broker就会记录TID,并生成一个新的组件__transaction_state用来保存TID的事务状态信息
当producer重启后,就会带着TID和新的PID向broker发起请求,当发现TID一致时
producer就会获取之前的PID,将覆盖掉新的PID,并获取上一次的事务状态信息,从而继续上次工作
consumer事务相对于producer事务就弱一点,需要先确保consumer的消费和提交位置为一致且具有事务功能,才能保证数据的完整,不然会造成数据的丢失或重复

## 21.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？

拦截器>序列化器>分区器

## 22.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？

![在这里插入图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20190918205837441.png)
使用两个线程:
main线程和sender线程
main线程会依次经过拦截器,序列化器,分区器将数据发送到RecourdAccumlator(线程共享变量)
再由sender线程从RecourdAccumlator中拉取数据发送到kafka broker
相关参数：
batch.size：只有数据积累到batch.size之后，sender才会发送数据。
linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。

## 23.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？

offset + 1
图示:
![在这里插入图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20190918211133620.png)
生产者发送数据offset是从0开始的
![在这里插入图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20190918211237148.png)
消费者消费的数据offset是从offset+1开始的

## 快捷启动Kafka的Python脚本代码

启动单个Kafka

[StartKafka.py3](https://my.oschina.net/jallenkwong/blog/StartKafka.py3)

```python
# -*- coding: utf-8 -*
import os
import time
import subprocess

startZooKeeperServerCmd = 'start bin\windows\zookeeper-server-start.bat config\zookeeper.properties'

startKafkaServerCmd = 'set JMX_PORT=9999 & start bin\windows\kafka-server-start.bat config\server.properties'

print('Starting ZooKeeper Server...')
subprocess.Popen(startZooKeeperServerCmd, shell=True)
time.sleep(10)

# 启动10s后, 轮询 2181端口是否启用
print('Polling...')
startKafkaFalg = False

#每次轮询间隔5秒
interval = 5
count = 6

while count > 0:
    tmpFile = os.popen('netstat -na','r')
    breakWhileFlag = False
    for line in tmpFile.readlines():
        if line.startswith('  TCP    0.0.0.0:2181'):
            breakWhileFlag = True
            break
    print("Not yet.")
    if breakWhileFlag:
        print("It's Ok.")
        startKafkaFalg = True
        break
    else:
        count -= 1
        time.sleep(interval)

if startKafkaFalg:
    time.sleep(interval)
    print("Starting the Kafka .")
    subprocess.Popen(startKafkaServerCmd, shell=True)
else:
    print("Something wrong ...")
    input()#raw_input()
```

### 启动三个Kafka

[StartKafkaCluster.py3](https://my.oschina.net/jallenkwong/blog/StartKafkaCluster.py3)

```python
# -*- coding: utf-8 -*
import os, time, subprocess

startZooKeeperServerCmd = 'start bin\windows\zookeeper-server-start.bat config\zookeeper.properties'

startKafkaServerCmd = 'set JMX_PORT=%d & start bin\windows\kafka-server-start.bat config\%s'

print('Starting ZooKeeper Server...')
subprocess.Popen(startZooKeeperServerCmd, shell=True)
time.sleep(10)

zooKeeperPortNumber = 2181
kafkaPortNumber = 9092
kafkaPortNumber2 = 9093
kafkaPortNumber3 = 9094

kafkaJmxPortNumber = 9997
kafkaJmxPortNumber2 = 9998
kafkaJmxPortNumber3 = 9999

def polling(portNumber, interval = 5, count = 10):
    while count > 0:
        tmpFile = os.popen('netstat -na','r')
        portNumberStr = str(portNumber)
        print("Polling the port: " + portNumberStr)
        for line in tmpFile.readlines():
            if line.startswith('  TCP    0.0.0.0:' + portNumberStr) or line.startswith('  TCP    127.0.0.1:' + portNumberStr):
                return True
        print("Not yet. " + str(portNumber))
        count -= 1
        time.sleep(interval)
    print("Polling the port: " + portNumberStr + " unsuccessfully.")
    return False

if polling(zooKeeperPortNumber):
    print("Starting the Kafka cluster...")
    subprocess.Popen(startKafkaServerCmd % (kafkaJmxPortNumber, 'server.properties'), shell=True)

    if polling(kafkaPortNumber):
        subprocess.Popen(startKafkaServerCmd % (kafkaJmxPortNumber2, 'server-1.properties'), shell=True)

    if polling(kafkaPortNumber2):
        subprocess.Popen(startKafkaServerCmd % (kafkaJmxPortNumber3, 'server-2.properties'), shell=True)
else:
    print("Something wrong ...")
    input()#raw_input()
```

### [详细解析kafka之kafka分区和副本](https://www.cnblogs.com/listenfwind/p/12465409.html)

kafka有三层形式，kafka有多个主题，每个主题有多个分区，每个分区又有多条消息。

当然多分区就意味着每条消息都难以按照顺序存储，那么是不是意味着这样的业务场景kafka就无能为力呢？不是的，**最简单的做法可以使用单个分区，单个分区，所有消息自然都顺序写入到一个分区中，就跟顺序队列一样了**。而复杂些的，还有其他办法，**那就是使用按消息键，将需要顺序保存的消息存储的单独的分区，其他消息存储其他分区，这个在下面会介绍**。

我们可以通过replication-factor指定创建topic时候所创建的分区数。

> bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test

比如这里就是创建了1个分区，的主题。值得注意的是，还有一种创建主题的方法，是使用zookeeper参数的，那种是比较旧的创建方法，这里是使用bootstrap参数的。

### 1.2 分区写入策略

所谓分区写入策略，即是生产者将数据写入到kafka主题后，kafka如何将数据分配到不同分区中的策略。

常见的有三种策略，轮询策略，随机策略，和按键保存策略。其中轮询策略是默认的分区策略，而随机策略则是较老版本的分区策略，不过由于其分配的均衡性不如轮询策略，故而后来改成了轮询策略为默认策略。

#### 轮询策略

所谓轮询策略，即按顺序轮流将每条数据分配到每个分区中。

举个例子，假设主题test有三个分区，分别是分区A，分区B和分区C。那么主题对接收到的第一条消息写入A分区，第二条消息写入B分区，第三条消息写入C分区，第四条消息则又写入A分区，依此类推。

轮询策略是默认的策略，故而也是使用最频繁的策略，它能最大限度保证所有消息都平均分配到每一个分区。除非有特殊的业务需求，否则使用这种方式即可。

#### 随机策略

随机策略，也就是每次都随机地将消息分配到每个分区。其实大概就是先得出分区的数量，然后每次获取一个随机数，用该随机数确定消息发送到哪个分区。

在比较早的版本，默认的分区策略就是随机策略，但其实使用随机策略也是为了更好得将消息均衡写入每个分区。但后来发现对这一需求而言，轮询策略的表现更优，所以社区后来的默认策略就是轮询策略了。

#### 按键保存策略

按键保存策略，就是当生产者发送数据的时候，可以指定一个key，计算这个key的hashCode值，按照hashCode的值对不同消息进行存储。

至于要如何实现，那也简单，只要让生产者发送的时候指定key就行。欸刚刚不是说默认的是轮询策略吗？其实啊，kafka默认是实现了两个策略，没指定key的时候就是轮询策略，有的话那激素按键保存策略了。

上面有说到一个场景，那就是要顺序发送消息到kafka。前面提到的方案是让所有数据存储到一个分区中，但其实更好的做法，就是使用这种按键保存策略。

让需要顺序存储的数据都指定相同的键，而不需要顺序存储的数据指定不同的键，这样一来，即实现了顺序存储的需求，又能够享受到kafka多分区的优势，岂不美哉。

### 1.3 实现自定义分区

说了这么多，那么到底要如何自定义分区呢？

kafka提供了两种让我们自己选择分区的方法，第一种是在发送producer的时候，在ProducerRecord中直接指定，但需要知道具体发送的分区index，所以并不推荐。

第二种则是需要实现Partitioner.class类，并重写类中的partition(String topic, Object key, byte[] keyBytes,Object value, byte[] valueBytes, Cluster cluster) 方法。后面在生成kafka producer客户端的时候直接指定新的分区类就可以了。

```
package kafkaconf;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ThreadLocalRandom;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.PartitionInfo;


public class MyParatitioner implements Partitioner {
    @Override
    public void configure(Map<String, ?> configs) {
    }

    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
                         Object value, byte[] valueBytes, Cluster cluster) {
        //key不能空，如果key为空的会通过轮询的方式 选择分区
        if(keyBytes == null || (!(key instanceof String))){
            throw new RuntimeException("key is null");
        }
        //获取分区列表
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);

        //以下是上述各种策略的实现，不能共存
        //随机策略
        return ThreadLocalRandom.current().nextInt(partitions.size());

        //按消息键保存策略
        return Math.abs(key.hashCode()) % partitions.size();

        //自定义分区策略, 比如key为123的消息，选择放入最后一个分区
        if(key.toString().equals("123")){
            return partitions.size()-1;
        }else{
            //否则随机
            ThreadLocalRandom.current().nextInt(partitions.size());
        }
    }

    @Override
    public void close() {
    }
}
```

然后需要在生成kafka producer客户端的时候指定该类就行：

```
    val properties = new Properties()
	......
	props.put("partitioner.class", "kafkaconf.MyParatitioner");  //主要这个配置指定分区类
	......其他配置
	val producer = new KafkaProducer[String, String](properties)
```

# 2.kafka副本机制

说完了分区，再来说说副本。先说说副本的基本内容，在kafka中，每个主题可以有多个分区，每个分区又可以有多个副本。这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。

多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。

这里通过问题来整理这部分内容。

#### kafka的副本都有哪些作用？

在kafka中，实现副本的目的就是冗余备份，且仅仅是冗余备份，所有的读写请求都是由leader副本进行处理的。follower副本仅有一个功能，那就是从leader副本拉取消息，尽量让自己跟leader副本的内容一致。

#### 说说follower副本为什么不对外提供服务？

这个问题本质上是对性能和一致性的取舍。试想一下，如果follower副本也对外提供服务那会怎么样呢？首先，性能是肯定会有所提升的。但同时，会出现一系列问题。类似数据库事务中的幻读，脏读。

比如你现在写入一条数据到kafka主题a，消费者b从主题a消费数据，却发现消费不到，因为消费者b去读取的那个分区副本中，最新消息还没写入。而这个时候，另一个消费者c却可以消费到最新那条数据，因为它消费了leader副本。

看吧，为了提高那么些性能而导致出现数据不一致问题，那显然是不值得的。

#### leader副本挂掉后，如何选举新副本？

如果你对zookeeper选举机制有所了解，就知道zookeeper每次leader节点挂掉时，都会通过内置id，来选举处理了最新事务的那个follower节点。

从结果上来说，kafka分区副本的选举也是类似的，都是选择最新的那个follower副本，但它是通过一个In-sync（ISR）副本集合实现。

kafka会将与leader副本保持同步的副本放到ISR副本集合中。当然，leader副本是一直存在于ISR副本集合中的，在某些特殊情况下，ISR副本中甚至只有leader一个副本。

当leader挂掉时，kakfa通过zookeeper感知到这一情况，在ISR副本中选取新的副本成为leader，对外提供服务。

但这样还有一个问题，前面提到过，有可能ISR副本集合中，只有leader，当leader副本挂掉后，ISR集合就为空，这时候怎么办呢？这时候如果设置unclean.leader.election.enable参数为true，那么kafka会在非同步，也就是不在ISR副本集合中的副本中，选取出副本成为leader，但这样意味这消息会丢失，这又是可用性和一致性的一个取舍了。

#### ISR副本集合保存的副本的条件是什么？

上面一直说ISR副本集合中的副本就是和leader副本是同步的，那这个同步的标准又是什么呢？

答案其实跟一个参数有关：replica.lag.time.max.ms。

前面说到follower副本的任务，就是从leader副本拉取消息，如果持续拉取速度慢于leader副本写入速度，慢于时间超过replica.lag.time.max.ms后，它就变成“非同步”副本，就会被踢出ISR副本集合中。但后面如何follower副本的速度慢慢提上来，那就又可能会重新加入ISR副本集合中了。

#### producer的acks参数

前面说了那么多理论的知识，那么就可以来看看如何在实际应用中使用这些知识。

跟副本关系最大的，那自然就是acks机制，acks决定了生产者如何在性能与数据可靠之间做取舍。

配置acks的代码其实很简单，只需要在新建producer的时候多加一个配置：

```
    val properties = new Properties()
	......
	props.put("acks", "0/1/-1");  //配置acks，有三个可选值
	......其他配置
	val producer = new KafkaProducer[String, String](properties)
```

acks这个配置可以指定三个值，分别是0，1和-1。我们分别来说三者代表什么：

- acks为0：这意味着producer发送数据后，不会等待broker确认，直接发送下一条数据，性能最快
- acks为1：为1意味着producer发送数据后，需要等待leader副本确认接收后，才会发送下一条数据，性能中等
- acks为-1：这个代表的是all，意味着发送的消息写入所有的ISR集合中的副本（注意不是全部副本）后，才会发送下一条数据，性能最慢，但可靠性最强

还有一点值得一提，kafka有一个配置参数，min.insync.replicas，默认是1（也就是只有leader，实际生产应该调高），该属性规定了最小的ISR数。这意味着当acks为-1（即all）的时候，这个参数规定了必须写入的ISR集中的副本数，如果没达到，那么producer会产生异常。

[Kafka学习笔记](https://my.oschina.net/jallenkwong/blog/4449224)https://my.oschina.net/jallenkwong/blog/4449224

# Kafka常见面试题

**1 什么是kafka**

> Kafka是分布式发布-订阅消息系统，它最初是由LinkedIn公司开发的，之后成为Apache项目的一部分，Kafka是一个分布式，可划分的，冗余备份的持久性的日志服务，它主要用于处理流式数据。

**2 为什么要使用 kafka，为什么要使用消息队列**

> 缓冲和削峰：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。
>
> 解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。
>
> 冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。
>
> 健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。
>
> 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。

**3.Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么**

> ISR:In-Sync Replicas 副本同步队列
> AR:Assigned Replicas 所有副本
> ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。

**4.kafka中的broker 是干什么的**

> broker 是消息的代理，Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉取指定Topic的消息，然后进行业务处理，broker在中间起到一个代理保存消息的中转站。

**5.kafka中的 zookeeper 起到什么作用，可以不用zookeeper么**

> zookeeper 是一个分布式的协调组件，早期版本的kafka用zk做meta信息存储，consumer的消费状态，group的管理以及 offset的值。考虑到zk本身的一些因素以及整个架构较大概率存在单点问题，新版本中逐渐弱化了zookeeper的作用。新的consumer使用了kafka内部的group coordination协议，也减少了对zookeeper的依赖，
>
> 但是broker依然依赖于ZK，zookeeper 在kafka中还用来选举controller 和 检测broker是否存活等等。

**6.kafka follower如何与leader同步数据**

> Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。完全同步复制要求All Alive Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下，如果leader挂掉，会丢失数据，kafka使用ISR的方式很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，而且Leader充分利用磁盘顺序读以及send file(zero copy)机制，这样极大的提高复制性能，内部批量写磁盘，大幅减少了Follower与Leader的消息量差。

**7.什么情况下一个 broker 会从 isr中踢出去**

> leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，而且是由leader动态维护 ，如果一个follower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除 。

**8.kafka 为什么那么快**

> - Cache Filesystem Cache PageCache缓存
> - 顺序写 由于现代的操作系统提供了预读和写技术，磁盘的顺序写大多数情况下比随机写内存还要快。
> - Zero-copy 零拷技术减少拷贝次数
> - Batching of Messages 批量量处理。合并小的请求，然后以流的方式进行交互，直顶网络上限。
> - Pull 拉模式 使用拉模式进行消息的获取消费，与消费端处理能力相符。

**9.kafka producer如何优化打入速度**

> - 增加线程
> - 提高 batch.size
> - 增加更多 producer 实例
> - 增加 partition 数
> - 设置 acks=-1 时，如果延迟增大：可以增大 num.replica.fetchers（follower 同步数据的线程数）来调解；
> - 跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。

**10.kafka producer 打数据，ack  为 0， 1， -1 的时候代表啥， 设置 -1 的时候，什么情况下，leader 会认为一条消息 commit了**

> 1. 1（默认） 数据发送到Kafka后，经过leader成功接收消息的的确认，就算是发送成功了。在这种情况下，如果leader宕机了，则会丢失数据。
> 2. 0 生产者将数据发送出去就不管了，不去等待任何返回。这种情况下数据传输效率最高，但是数据可靠性确是最低的。
> 3. -1 producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。当ISR中所有Replica都向Leader发送ACK时，leader才commit，这时候producer才能认为一个请求中的消息都commit了。

**11.kafka  unclean 配置代表啥，会对 spark streaming 消费有什么影响**

> unclean.leader.election.enable 为true的话，意味着非ISR集合的broker 也可以参与选举，这样有可能就会丢数据，spark streaming在消费过程中拿到的 end offset 会突然变小，导致 spark streaming job挂掉。如果unclean.leader.election.enable参数设置为true，就有可能发生数据丢失和数据不一致的情况，Kafka的可靠性就会降低；而如果unclean.leader.election.enable参数设置为false，Kafka的可用性就会降低。

**12.如果leader crash时，ISR为空怎么办**

> kafka在Broker端提供了一个配置参数：unclean.leader.election,这个参数有两个值：
> true（默认）：允许不同步副本成为leader，由于不同步副本的消息较为滞后，此时成为leader，可能会出现消息不一致的情况。
> false：不允许不同步副本成为leader，此时如果发生ISR列表为空，会一直等待旧leader恢复，降低了可用性。

**13.kafka的message格式是什么样的**

> 一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成
>
> header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。
>
> 当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，
>
> 比如是否压缩、压缩格式等等);如果magic的值为0，那么不存在attributes属性
>
> body是由N个字节构成的一个消息体，包含了具体的key/value消息

**14.kafka中consumer group 是什么概念**

> 同样是逻辑上的概念，是Kafka实现单播和广播两种消息模型的手段。同一个topic的数据，会广播给不同的group；同一个group中的worker，只有一个worker能拿到这个数据。换句话说，对于同一个topic，每个group都可以拿到同样的所有数据，但是数据进入group后只能被其中的一个worker消费。group内的worker可以使用多线程或多进程来实现，也可以将进程分散在多台机器上，worker的数量通常不超过partition的数量，且二者最好保持整数倍关系，因为Kafka在设计时假定了一个partition只能被一个worker消费（同一group内）。

**15.Kafka中的消息是否会丢失和重复消费？**

> 要确定Kafka的消息是否丢失或重复，从两个方面分析入手：消息发送和消息消费。
>
> **1、消息发送**
>
> ​     Kafka消息发送有两种方式：同步（sync）和异步（async），默认是同步方式，可通过producer.type属性进行配置。Kafka通过配置request.required.acks属性来确认消息的生产：
>
> 1. *0---表示不进行消息接收是否成功的确认；*
> 2. *1---表示当Leader接收成功时确认；*
> 3. *-1---表示Leader和Follower都接收成功时确认；*
>
> 综上所述，有6种消息生产的情况，下面分情况来分析消息丢失的场景：
>
> （1）acks=0，不和Kafka集群进行消息接收确认，则当网络异常、缓冲区满了等情况时，**消息可能丢失**；
>
> （2）acks=1、同步模式下，只有Leader确认接收成功后但挂掉了，副本没有同步，**数据可能丢失**；
>
> **2、消息消费**
>
> Kafka消息消费有两个consumer接口，Low-level API和High-level API：
>
> 1. Low-level API：消费者自己维护offset等值，可以实现对Kafka的完全控制；
> 2. High-level API：封装了对parition和offset的管理，使用简单；
>
> 如果使用高级接口High-level API，可能存在一个问题就是当消息消费者从集群中把消息取出来、并提交了新的消息offset值后，还没来得及消费就挂掉了，那么下次再消费时之前没消费成功的消息就“*诡异*”的消失了；
>
> **解决办法**：
>
> ​    针对消息丢失：同步模式下，确认机制设置为-1，即让消息写入Leader和Follower之后再确认消息发送成功；异步模式下，为防止缓冲区满，可以在配置文件设置不限制阻塞超时时间，当缓冲区满时让生产者一直处于阻塞状态；
>
> ​    针对消息重复：将消息的唯一标识保存到外部介质中，每次消费时判断是否处理过即可。
>
> 消息重复消费及解决参考：https://www.javazhiyin.com/22910.html

**16.为什么Kafka不支持读写分离？**

> 在 Kafka 中，生产者写入消息、消费者读取消息的操作都是与 leader 副本进行交互的，从 而实现的是一种**主写主读**的生产消费模型。
>
> Kafka 并不支持主写从读，因为主写从读有 2 个很明 显的缺点:
>
> - (1)**数据一致性问题**。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。
> - (2)**延时问题**。类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经 历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节 点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。

**17.Kafka中是怎么体现消息顺序性的？**

> kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。
> 整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.

**18.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?**

> offset+1

**19.kafka如何实现延迟队列？**

> Kafka并没有使用JDK自带的Timer或者DelayQueue来实现延迟的功能，而是**基于时间轮自定义了一个用于实现延迟功能的定时器（SystemTimer）**。JDK的Timer和DelayQueue插入和删除操作的平均时间复杂度为O(nlog(n))，并不能满足Kafka的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为**O(1)**。时间轮的应用并非Kafka独有，其应用场景还有很多，在Netty、Akka、Quartz、Zookeeper等组件中都存在时间轮的踪影。
>
> 底层使用数组实现，数组中的每个元素可以存放一个TimerTaskList对象。TimerTaskList是一个环形双向链表，在其中的链表项TimerTaskEntry中封装了真正的定时任务TimerTask.
>
> Kafka中到底是怎么推进时间的呢？Kafka中的定时器借助了JDK中的DelayQueue来协助推进时间轮。具体做法是对于每个使用到的TimerTaskList都会加入到DelayQueue中。**Kafka中的TimingWheel专门用来执行插入和删除TimerTaskEntry的操作，而DelayQueue专门负责时间推进的任务**。再试想一下，DelayQueue中的第一个超时任务列表的expiration为200ms，第二个超时任务为840ms，这里获取DelayQueue的队头只需要O(1)的时间复杂度。如果采用每秒定时推进，那么获取到第一个超时的任务列表时执行的200次推进中有199次属于“空推进”，而获取到第二个超时任务时有需要执行639次“空推进”，这样会无故空耗机器的性能资源，这里采用DelayQueue来辅助以少量空间换时间，从而做到了“精准推进”。Kafka中的定时器真可谓是“知人善用”，用TimingWheel做最擅长的任务添加和删除操作，而用DelayQueue做最擅长的时间推进工作，相辅相成。
>
> 参考：https://blog.csdn.net/u013256816/article/details/80697456

**20.Kafka中的事务是怎么实现的？**

> 参考：https://blog.csdn.net/u013256816/article/details/89135417

**21**.**Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？**

> **https://blog.csdn.net/yanshu2012/article/details/54894629**

# Kafka史上最详细原理总结

Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。

### 1.前言

消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。下面将从Kafka文件存储机制和物理结构角度，分析Kafka是如何实现高效文件存储，及实际应用效果。



####  **1.1  Kafka的特性:**

\- 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition, consumer group 对partition进行consume操作。

\- 可扩展性：kafka集群支持热扩展

\- 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失

\- 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）

\- 高并发：支持数千个客户端同时读写

 

#### **1.2  Kafka的使用场景：**

\- 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。

\- 消息系统：解耦和生产者和消费者、缓存消息等。

\- 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。

\- 运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。

\- 流式处理：比如spark streaming和storm

\- 事件源

 

#### **1.3  Kakfa的设计思想**

\- **Kakfa Broker Leader的选举：** Kakfa Broker集群受Zookeeper管理。所有的Kafka Broker节点一起去Zookeeper上注册一个临时节点，因为只有一个Kafka Broker会注册成功，其他的都会失败，所以这个成功在Zookeeper上注册临时节点的这个Kafka Broker会成为Kafka Broker Controller，其他的Kafka broker叫Kafka Broker follower。（这个过程叫Controller在ZooKeeper注册Watch）。这个Controller会监听其他的Kafka Broker的所有信息，如果这个kafka broker controller宕机了，在zookeeper上面的那个临时节点就会消失，此时所有的kafka broker又会一起去 Zookeeper上注册一个临时节点，因为只有一个Kafka Broker会注册成功，其他的都会失败，所以这个成功在Zookeeper上注册临时节点的这个Kafka Broker会成为Kafka Broker Controller，其他的Kafka broker叫Kafka Broker follower 。例如：一旦有一个broker宕机了，**这个kafka broker controller会读取该宕机broker上所有的partition在zookeeper上的状态**，**并选取ISR列表中的一个replica作为partition leader**（如果ISR列表中的replica全挂，选一个幸存的replica作为leader; 如果该partition的所有的replica都宕机了，则将新的leader设置为-1，等待恢复，等待ISR中的任一个Replica“活”过来，并且选它作为Leader；或选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader），这个broker宕机的事情，kafka controller也会通知zookeeper，zookeeper就会通知其他的kafka broker。

**这里曾经发生过一个bug，TalkingData使用Kafka0.8.1的时候，kafka controller在Zookeeper上注册成功后，它和Zookeeper通信的timeout时间是6s，也就是如果kafka controller如果有6s中没有和Zookeeper做心跳，那么Zookeeper就认为这个kafka controller已经死了，就会在Zookeeper上把这个临时节点删掉，那么其他Kafka就会认为controller已经没了，就会再次抢着注册临时节点，注册成功的那个kafka broker成为controller，然后，之前的那个kafka controller就需要各种shut down去关闭各种节点和事件的监听。但是当kafka的读写流量都非常巨大的时候，TalkingData的一个bug是，由于网络等原因，kafka controller和Zookeeper有6s中没有通信，于是重新选举出了一个新的kafka controller，但是原来的controller在shut down的时候总是不成功，这个时候producer进来的message由于Kafka集群中存在两个kafka controller而无法落地。导致数据淤积。**

这里曾经还有一个bug，TalkingData使用Kafka0.8.1的时候，当ack=0的时候，表示producer发送出去message，只要对应的kafka broker topic partition leader接收到的这条message，producer就返回成功，不管partition leader 是否真的成功把message真正存到kafka。当ack=1的时候，表示producer发送出去message，同步的把message存到对应topic的partition的leader上，然后producer就返回成功，partition leader异步的把message同步到其他partition replica上。当ack=all或-1，表示producer发送出去message，同步的把message存到对应topic的partition的leader和对应的replica上之后，才返回成功。但是如果某个kafka controlle **切换的时候，会导致partition leader的切换（老的** **kafka controller上面的partition leader会选举到其他的kafka broker上****）,但是这样就会导致丢数据。**

\- **Consumergroup：**各个consumer（consumer 线程）可以组成一个组（Consumer group ），partition中的每个message只能被 组（Consumer group ） 中的一个consumer（ consumer 线程 ）消费，如果一个message可以被多个 consumer（ consumer 线程 ） 消费的话，那么这些consumer必须在不同的组。Kafka不支持一个partition中的message由两个或两个以上的同一个consumer group下的consumer thread来处理，除非再启动一个新的consumer group。所以如果想同时对一个topic做消费的话，启动多个consumer group就可以了，但是要注意的是，这里的多个consumer的消费都必须是顺序读取partition里面的message，新启动的consumer默认从partition队列最头端最新的地方开始阻塞的读message。它不能像AMQ那样可以多个BET作为consumer去互斥的（for update悲观锁）并发处理message，这是因为多个BET去消费一个Queue中的数据的时候，由于要保证不能多个线程拿同一条message，所以就需要行级别悲观所（for update）,这就导致了consume的性能下降，吞吐量不够。而kafka为了保证吞吐量，只允许同一个consumer group下的一个consumer线程去访问一个partition。如果觉得效率不高的时候，可以加partition的数量来横向扩展，那么再加新的consumer thread去消费。如果想多个不同的业务都需要这个topic的数据，起多个consumer group就好了，大家都是顺序的读取message，offsite的值互不影响。这样没有锁竞争，充分发挥了横向的扩展性，吞吐量极高。这也就形成了分布式消费的概念。

  当启动一个consumer group去消费一个topic的时候，无论topic里面有多个少个partition，无论我们consumer group里面配置了多少个consumer thread，这个consumer group下面的所有consumer thread一定会消费全部的partition；即便这个consumer group下只有一个consumer thread，那么这个consumer thread也会去消费所有的partition。因此，最优的设计就是，consumer group下的consumer thread的数量等于partition数量，这样效率是最高的。

  同一partition的一条message只能被同一个Consumer Group内的一个Consumer消费。不能够一个consumer group的多个consumer同时消费一个partition。

  一个consumer group下，无论有多少个consumer，这个consumer group一定回去把这个topic下所有的partition都消费了。当consumer group里面的consumer数量小于这个topic下的partition数量的时候，如下图groupA,groupB，就会出现一个conusmer thread消费多个partition的情况，总之是这个topic下的partition都会被消费。如果 consumer group里面的consumer数量等于这个topic下的partition数量的时候，如下图groupC，此时效率是最高的，每个partition都有一个consumer thread去消费。当 consumer group里面的consumer数量大于这个topic下的partition数量的时候，如下图GroupD，就会有一个consumer thread空闲。因此，我们在设定consumer group的时候，只需要指明里面有几个consumer数量即可，无需指定对应的消费partition序号，consumer会自动进行rebalance。

  多个Consumer Group下的consumer可以消费同一条message，但是这种消费也是以o（1）的方式顺序的读取message去消费,，所以一定会重复消费这批message的，不能向AMQ那样多个BET作为consumer消费（对message加锁，消费的时候不能重复消费message）

\- **Consumer Rebalance的触发条件：**（1）Consumer增加或删除会触发 Consumer Group的Rebalance （2）Broker的增加或者减少都会触发 Consumer Rebalance

\- **Consumer：** Consumer处理partition里面的message的时候是o（1）顺序读取的。所以必须维护着上一次读到哪里的offsite信息。high level API,offset存于Zookeeper中，low level API的offset由自己维护。一般来说都是使用high level api的。Consumer的delivery gurarantee，默认是读完message先commmit再处理message，autocommit默认是true，这时候先commit就会更新offsite+1，一旦处理失败，offsite已经+1，这个时候就会丢message；也可以配置成读完消息处理再commit，这种情况下consumer端的响应就会比较慢的，需要等处理完才行。

一般情况下，一定是一个consumer group处理一个topic的message。Best Practice是这个consumer group里面consumer的数量等于topic里面partition的数量，这样效率是最高的，一个consumer thread处理一个partition。如果这个consumer group里面consumer的数量小于topic里面partition的数量，就会有consumer thread同时处理多个partition（这个是kafka自动的机制，我们不用指定），但是总之这个topic里面的所有partition都会被处理到的。。如果这个consumer group里面consumer的数量大于topic里面partition的数量，多出的consumer thread就会闲着啥也不干，剩下的是一个consumer thread处理一个partition，这就造成了资源的浪费，因为一个partition不可能被两个consumer thread去处理。 所以我们线上的分布式多个service服务，每个service里面的kafka consumer数量都小于对应的topic的partition数量，但是所有服务的consumer数量只和等于partition的数量，这是因为分布式service服务的所有consumer都来自一个consumer group，如果来自不同的consumer group就会处理重复的message了（同一个consumer group下的consumer不能处理同一个partition，不同的consumer group可以处理同一个topic，那么都是顺序处理message，一定会处理重复的。一般这种情况都是两个不同的业务逻辑，才会启动两个consumer group来处理一个topic）。

 

**如果producer的流量增大，当前的topic的parition数量=consumer数量，这时候的应对方式就是很想扩展：增加topic下的partition，同时增加这个consumer group下的consumer。**

 ![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107211358384)       

\- **Delivery Mode :** Kafka producer 发送message不用维护message的offsite信息，因为这个时候，offsite就相当于一个自增id，producer就尽管发送message就好了。而且Kafka与AMQ不同，AMQ大都用在处理业务逻辑上，而Kafka大都是日志，所以Kafka的producer一般都是大批量的batch发送message，向这个topic一次性发送一大批message，load balance到一个partition上，一起插进去，offsite作为自增id自己增加就好。但是Consumer端是需要维护这个partition当前消费到哪个message的offsite信息的，这个offsite信息，high level api是维护在Zookeeper上，low level api是自己的程序维护。（Kafka管理界面上只能显示high level api的consumer部分，因为low level api的partition offsite信息是程序自己维护，kafka是不知道的，无法在管理界面上展示 ）当使用high level api的时候，先拿message处理，再定时自动commit offsite+1（也可以改成手动）, 并且kakfa处理message是没有锁操作的。因此如果处理message失败，此时还没有commit offsite+1，当consumer thread重启后会重复消费这个message。但是作为高吞吐量高并发的实时处理系统，at least once的情况下，至少一次会被处理到，是可以容忍的。如果无法容忍，就得使用low level api来自己程序维护这个offsite信息，那么想什么时候commit offsite+1就自己搞定了。

 

\- **Topic & Partition：**Topic相当于传统消息系统MQ中的一个队列queue，producer端发送的message必须指定是发送到哪个topic，但是不需要指定topic下的哪个partition，因为kafka会把收到的message进行load balance，均匀的分布在这个topic下的不同的partition上（ hash(message) % [broker数量]  ）。物理上存储上，这个topic会分成一个或多个partition，每个partiton相当于是一个子queue。在物理结构上，每个partition对应一个物理的目录（文件夹），文件夹命名是[topicname]_[partition]_[序号]，一个topic可以有无数多的partition，根据业务需求和数据量来设置。在kafka配置文件中可随时更高num.partitions参数来配置更改topic的partition数量，在 创建Topic时通过参数指定parittion数量。 Topic创建之后通过Kafka提供的工具也可以修改partiton数量。

  一般来说，（1）一个Topic的Partition数量大于等于Broker的数量， 可以提高吞吐率。（2）同一个Partition的Replica尽量分散到不同的机器， 高可用。

 **当add a new partition的时候**，partition里面的message不会重新进行分配，原来的partition里面的message数据不会变，新加的这个partition刚开始是空的，随后进入这个topic的message就会重新参与所有partition的load balance

\- **Partition Replica：**每个partition可以在其他的kafka broker节点上存副本，以便某个kafka broker节点宕机不会影响这个kafka集群。存replica副本的方式是按照kafka broker的顺序存。例如有5个kafka broker节点，某个topic有3个partition，每个partition存2个副本，那么partition1存broker1,broker2，partition2存broker2,broker3。。。以此类推**（replica副本数目不能大于kafka broker节点的数目，否则报错。这里的replica数其实就是partition的副本总数，其中包括一个leader，其他的就是copy副本）**。这样如果某个broker宕机，其实整个kafka内数据依然是完整的。但是，replica副本数越高，系统虽然越稳定，但是回来带资源和性能上的下降；replica副本少的话，也会造成系统丢数据的风险。

 （1）怎样传送消息：producer先把message发送到partition leader，再由leader发送给其他partition follower。（如果让producer发送给每个replica那就太慢了）

 （2） 在向Producer发送ACK前需要保证有多少个Replica已经收到该消息：根据ack配的个数而定

 （3） 怎样处理某个Replica不工作的情况：如果这个部工作的partition replica不在ack列表中，就是producer在发送消息到partition leader上，partition leader向partition follower发送message没有响应而已，这个不会影响整个系统，也不会有什么问题。如果这个不工作的partition replica在ack列表中的话，producer发送的message的时候会等待这个不工作的partition replca写message成功，但是会等到time out，然后返回失败因为某个ack列表中的partition replica没有响应，此时kafka会自动的把这个部工作的partition replica从ack列表中移除，以后的producer发送message的时候就不会有这个ack列表下的这个部工作的partition replica了。 

 （4） 怎样处理Failed Replica恢复回来的情况：如果这个partition replica之前不在ack列表中，那么启动后重新受Zookeeper管理即可，之后producer发送message的时候，partition leader会继续发送message到这个partition follower上。如果这个 partition replica之前在ack列表中，此时重启后，需要把这个partition replica再手动加到ack列表中。（ack列表是手动添加的，出现某个部工作的partition replica的时候自动从ack列表中移除的）

\- **Partition leader与follower：**partition也有leader和follower之分。leader是主partition，producer写kafka的时候先写partition leader，再由partition leader push给其他的partition follower。partition leader与follower的信息受Zookeeper控制，一旦partition leader所在的broker节点宕机，zookeeper会冲其他的broker的partition follower上选择follower变为parition leader。

\- **Topic分配partition和partition replica的算法：**（1）将Broker（size=n）和待分配的Partition排序。（2） 将第i个Partition分配到第（i%n）个Broker上。（3） 将第i个Partition的第j个Replica分配到第（(i + j) % n）个Broker上

 

***\*- 消息投递可靠性\****

一个消息如何算投递成功，Kafka提供了三种模式：

\- 第一种是啥都不管，发送出去就当作成功，这种情况当然不能保证消息成功投递到broker；

\- 第二种是Master-Slave模型，只有当Master和所有Slave都接收到消息时，才算投递成功，这种模型提供了最高的投递可靠性，但是损伤了性能；

\- 第三种模型，即只要Master确认收到消息就算投递成功；实际使用时，根据应用特性选择，绝大多数情况下都会中和可靠性和性能选择第三种模型

 消息在broker上的可靠性，因为消息会持久化到磁盘上，所以如果正常stop一个broker，其上的数据不会丢失；但是如果不正常stop，可能会使存在页面缓存来不及写入磁盘的消息丢失，这可以通过配置flush页面缓存的周期、阈值缓解，但是同样会频繁的写磁盘会影响性能，又是一个选择题，根据实际情况配置。

 消息消费的可靠性，Kafka提供的是“At least once”模型，因为消息的读取进度由offset提供，offset可以由消费者自己维护也可以维护在zookeeper里，但是当消息消费后consumer挂掉，offset没有即时写回，就有可能发生重复读的情况，这种情况同样可以通过调整commit offset周期、阈值缓解，甚至消费者自己把消费和commit offset做成一个事务解决，但是如果你的应用不在乎重复消费，那就干脆不要解决，以换取最大的性能。

 

\- **Partition ack：**当ack=1，表示producer写partition leader成功后，broker就返回成功，无论其他的partition follower是否写成功。当ack=2，表示producer写partition leader和其他一个follower成功的时候， broker就返回成功，无论其他的partition follower是否写成功。当ack=-1 [parition的数量]的时候，表示只有producer全部写成功的时候，才算成功，kafka broker才返回成功信息。**这里需要注意的是，如果ack=1的时候，一旦有个broker宕机导致partition的follower和leader切换，会导致丢数据。**

  ![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107211558246)

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107211616996)

\- **message状态** ：在Kafka中，消息的状态被保存在consumer中，broker不会关心哪个消息被消费了被谁消费了，只记录一个offset值（指向partition中下一个要被消费的消息位置），这就意味着如果consumer处理不好的话，broker上的一个消息可能会被消费多次。

\- **message持久化**：Kafka中会把消息持久化到本地文件系统中，并且保持o(1)极高的效率。我们众所周知IO读取是非常耗资源的性能也是最慢的，这就是为了数据库的瓶颈经常在IO上，需要换SSD硬盘的原因。但是Kafka作为吞吐量极高的MQ，却可以非常高效的message持久化到文件。这是因为Kafka是顺序写入o（1）的时间复杂度，速度非常快。也是高吞吐量的原因。由于message的写入持久化是顺序写入的，因此message在被消费的时候也是按顺序被消费的，保证partition的message是顺序消费的。一般的机器,单机每秒100k条数据。

\- **message有效期**：Kafka会长久保留其中的消息，以便consumer可以多次消费，当然其中很多细节是可配置的。

\- **Produer :** Producer向Topic发送message，不需要指定partition，直接发送就好了。kafka通过partition ack来控制是否发送成功并把信息返回给producer，producer可以有任意多的thread，这些kafka服务器端是不care的。Producer端的delivery guarantee默认是At least once的。也可以设置Producer异步发送实现At most once。Producer可以用主键幂等性实现Exactly once

\- **Kafka高吞吐量**： Kafka的高吞吐量体现在读写上，分布式并发的读和写都非常快，写的性能体现在以o(1)的时间复杂度进行顺序写入。读的性能 体现在以o(1)的时间复杂度进行顺序读取， 对topic进行partition分区，consume group中的consume线程可以以很高能性能进行顺序读。

\- Kafka delivery guarantee(message传送保证)：（1）At most once消息可能会丢，绝对不会重复传输；（2）At least once 消息绝对不会丢，但是可能会重复传输；（3）Exactly once每条信息肯定会被传输一次且仅传输一次，这是用户想要的。

\- **批量发送**：Kafka支持以消息集合为单位进行批量发送，以提高push效率。

\- **push-and-pull** : Kafka中的Producer和consumer采用的是push-and-pull模式，即Producer只管向broker push消息，consumer只管从broker pull消息，两者对消息的生产和消费是异步的。

\- **Kafka集群中broker之间的关系**：不是主从关系，各个broker在集群中地位一样，我们可以随意的增加或删除任何一个broker节点。

\- **负载均衡方面**： Kafka提供了一个 metadata API来管理broker之间的负载（对Kafka0.8.x而言，对于0.7.x主要靠zookeeper来实现负载均衡）。

\- **同步异步**：Producer采用异步push方式，极大提高Kafka系统的吞吐率（可以通过参数控制是采用同步还是异步方式）。

\- **分区机制partition**：Kafka的broker端支持消息分区partition，Producer可以决定把消息发到哪个partition，在一个 partition 中message的顺序就是Producer发送消息的顺序，一个topic中可以有多个partition，具体partition的数量是可配置的。partition的概念使得kafka作为MQ可以横向扩展，吞吐量巨大。partition可以设置replica副本，replica副本存在不同的kafka broker节点上，第一个partition是leader,其他的是follower，message先写到partition leader上，再由partition leader push到parition follower上。所以说kafka可以水平扩展，也就是扩展partition。

\- **离线数据装载**：Kafka由于对可拓展的数据持久化的支持，它也非常适合向Hadoop或者数据仓库中进行数据装载。

\- **实时数据与离线数据：**kafka既支持离线数据也支持实时数据，因为kafka的message持久化到文件，并可以设置有效期，因此可以把kafka作为一个高效的存储来使用，可以作为离线数据供后面的分析。当然作为分布式实时消息系统，大多数情况下还是用于实时的数据处理的，但是当cosumer消费能力下降的时候可以通过message的持久化在淤积数据在kafka。

\- **插件支持**：现在不少活跃的社区已经开发出不少插件来拓展Kafka的功能，如用来配合Storm、Hadoop、flume相关的插件。

\- **解耦**:  相当于一个MQ，使得Producer和Consumer之间异步的操作，系统之间解耦

\- **冗余**:  replica有多个副本，保证一个broker node宕机后不会影响整个服务

\- **扩展性**:  broker节点可以水平扩展，partition也可以水平增加，partition replica也可以水平增加

\- **峰值**:  在访问量剧增的情况下，kafka水平扩展, 应用仍然需要继续发挥作用

\- **可恢复性**:  系统的一部分组件失效时，由于有partition的replica副本，不会影响到整个系统。

\- **顺序保证性**：由于kafka的producer的写message与consumer去读message都是顺序的读写，保证了高效的性能。

\- **缓冲**：由于producer那面可能业务很简单，而后端consumer业务会很复杂并有数据库的操作，因此肯定是producer会比consumer处理速度快，如果没有kafka，producer直接调用consumer，那么就会造成整个系统的处理速度慢，加一层kafka作为MQ，可以起到缓冲的作用。

\- **异步通信**：作为MQ，Producer与Consumer异步通信

## 2.Kafka文件存储机制

**2.1 Kafka部分名词解释如下：**

 

   Kafka中发布订阅的对象是topic。我们可以为每类数据创建一个topic，把向topic发布消息的客户端称作producer，从topic订阅消息的客户端称作consumer。Producers和consumers可以同时从多个topic读写数据。一个kafka集群由一个或多个broker服务器组成，它负责持久化和备份具体的kafka消息。

- **Broker**：Kafka节点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。
- **Topic**：一类消息，消息存放的目录即主题，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。
- **Partition**：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列
- **Segment**：partition物理上由多个segment组成，每个Segment存着message信息
- **Producer** : 生产message发送到topic
- **Consumer** : 订阅topic消费message, consumer作为一个线程来消费
- **Consumer Group**：一个Consumer Group包含多个consumer, 这个是预先在配置文件中配置好的。各个consumer（consumer 线程）可以组成一个组（Consumer group ），partition中的每个message只能被组（Consumer group ） 中的一个consumer（consumer 线程 ）消费，如果一个message可以被多个consumer（consumer 线程 ） 消费的话，那么这些consumer必须在不同的组。Kafka不支持一个partition中的message由两个或两个以上的consumer thread来处理，即便是来自不同的consumer group的也不行。它不能像AMQ那样可以多个BET作为consumer去处理message，这是因为多个BET去消费一个Queue中的数据的时候，由于要保证不能多个线程拿同一条message，所以就需要行级别悲观所（for update）,这就导致了consume的性能下降，吞吐量不够。而kafka为了保证吞吐量，只允许一个consumer线程去访问一个partition。如果觉得效率不高的时候，可以加partition的数量来横向扩展，那么再加新的consumer thread去消费。这样没有锁竞争，充分发挥了横向的扩展性，吞吐量极高。这也就形成了分布式消费的概念。
   
- **2.2 kafka一些原理概念**

1.持久化

kafka使用文件存储消息(append only log),这就直接决定kafka在性能上严重依赖文件系统的本身特性.且无论任何OS下,对文件系统本身的优化是非常艰难的.文件缓存/直接内存映射等是常用的手段.因为kafka是对日志文件进行append操作,因此磁盘检索的开支是较小的;同时为了减少磁盘写入的次数,broker会将消息暂时buffer起来,当消息的个数(或尺寸)达到一定阀值时,再flush到磁盘,这样减少了磁盘IO调用的次数.对于kafka而言,较高性能的磁盘,将会带来更加直接的性能提升.

 

2.性能

除磁盘IO之外,我们还需要考虑网络IO,这直接关系到kafka的吞吐量问题.kafka并没有提供太多高超的技巧;对于producer端,可以将消息buffer起来,当消息的条数达到一定阀值时,批量发送给broker;对于consumer端也是一样,批量fetch多条消息.不过消息量的大小可以通过配置文件来指定.对于kafka broker端,似乎有个sendfile系统调用可以潜在的提升网络IO的性能:将文件的数据映射到系统内存中,socket直接读取相应的内存区域即可,而无需进程再次copy和交换(这里涉及到"磁盘IO数据"/"内核内存"/"进程内存"/"网络缓冲区",多者之间的数据copy).

其实对于producer/consumer/broker三者而言,CPU的开支应该都不大,因此启用消息压缩机制是一个良好的策略;压缩需要消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑.可以将任何在网络上传输的消息都经过压缩.kafka支持gzip/snappy等多种压缩方式.

 

3.负载均衡

kafka集群中的任何一个broker,都可以向producer提供metadata信息,这些metadata中包含"集群中存活的servers列表"/"partitions leader列表"等信息(请参看zookeeper中的节点信息). 当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接;消息由producer直接通过socket发送到broker,中间不会经过任何"路由层".

异步发送，将多条消息暂且在客户端buffer起来,并将他们批量发送到broker;小数据IO太多,会拖慢整体的网络延迟,批量延迟发送事实上提升了网络效率;不过这也有一定的隐患,比如当producer失效时,那些尚未发送的消息将会丢失。

 

4.Topic模型

其他JMS实现,消息消费的位置是有prodiver保留,以便避免重复发送消息或者将没有消费成功的消息重发等,同时还要控制消息的状态.这就要求JMS broker需要太多额外的工作.在kafka中,partition中的消息只有一个consumer在消费,且不存在消息状态的控制,也没有复杂的消息确认机制,可见kafka broker端是相当轻量级的.当消息被consumer接收之后,consumer可以在本地保存最后消息的offset,并间歇性的向zookeeper注册offset.由此可见,consumer客户端也很轻量级。

kafka中consumer负责维护消息的消费记录,而broker则不关心这些,这种设计不仅提高了consumer端的灵活性,也适度的减轻了broker端设计的复杂度;这是和众多JMS prodiver的区别.此外,kafka中消息ACK的设计也和JMS有很大不同,kafka中的消息是批量(通常以消息的条数或者chunk的尺寸为单位)发送给consumer,当消息消费成功后,向zookeeper提交消息的offset,而不会向broker交付ACK.或许你已经意识到,这种"宽松"的设计,将会有"丢失"消息/"消息重发"的危险.

 

5.消息传输一致

Kafka提供3种消息传输一致性语义：最多1次，最少1次，恰好1次。

最少1次：可能会重传数据，有可能出现数据被重复处理的情况;

最多1次：可能会出现数据丢失情况;

恰好1次：并不是指真正只传输1次，只不过有一个机制。确保不会出现“数据被重复处理”和“数据丢失”的情况。

 

at most once: 消费者fetch消息,然后保存offset,然后处理消息;当client保存offset之后,但是在消息处理过程中consumer进程失效(crash),导致部分消息未能继续处理.那么此后可能其他consumer会接管,但是因为offset已经提前保存,那么新的consumer将不能fetch到offset之前的消息(尽管它们尚没有被处理),这就是"at most once".

at least once: 消费者fetch消息,然后处理消息,然后保存offset.如果消息处理成功之后,但是在保存offset阶段zookeeper异常或者consumer失效,导致保存offset操作未能执行成功,这就导致接下来再次fetch时可能获得上次已经处理过的消息,这就是"at least once".

"Kafka Cluster"到消费者的场景中可以采取以下方案来得到“恰好1次”的一致性语义：

最少1次＋消费者的输出中额外增加已处理消息最大编号：由于已处理消息最大编号的存在，不会出现重复处理消息的情况。

 

6.副本

kafka中,replication策略是基于partition,而不是topic;kafka将每个partition数据复制到多个server上,任何一个partition有一个leader和多个follower(可以没有);备份的个数可以通过broker配置文件来设定。leader处理所有的read-write请求,follower需要和leader保持同步.Follower就像一个"consumer",消费消息并保存在本地日志中;leader负责跟踪所有的follower状态,如果follower"落后"太多或者失效,leader将会把它从replicas同步列表中删除.当所有的follower都将一条消息保存成功,此消息才被认为是"committed",那么此时consumer才能消费它,这种同步策略,就要求follower和leader之间必须具有良好的网络环境.即使只有一个replicas实例存活,仍然可以保证消息的正常发送和接收,只要zookeeper集群存活即可.

选择follower时需要兼顾一个问题,就是新leader server上所已经承载的partition leader的个数,如果一个server上有过多的partition leader,意味着此server将承受着更多的IO压力.在选举新leader,需要考虑到"负载均衡",partition leader较少的broker将会更有可能成为新的leader.

 

7.log

每个log entry格式为"4个字节的数字N表示消息的长度" + "N个字节的消息内容";每个日志都有一个offset来唯一的标记一条消息,offset的值为8个字节的数字,表示此消息在此partition中所处的起始位置..每个partition在物理存储层面,有多个log file组成(称为segment).segment file的命名为"最小offset".kafka.例如"00000000000.kafka";其中"最小offset"表示此segment中起始消息的offset.

获取消息时,需要指定offset和最大chunk尺寸,offset用来表示消息的起始位置,chunk size用来表示最大获取消息的总长度(间接的表示消息的条数).根据offset,可以找到此消息所在segment文件,然后根据segment的最小offset取差值,得到它在file中的相对位置,直接读取输出即可.

 

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107211730692)

8.分布式

kafka使用zookeeper来存储一些meta信息,并使用了zookeeper watch机制来发现meta信息的变更并作出相应的动作(比如consumer失效,触发负载均衡等)

Broker node registry: 当一个kafka broker启动后,首先会向zookeeper注册自己的节点信息(临时znode),同时当broker和zookeeper断开连接时,此znode也会被删除.

Broker Topic Registry: 当一个broker启动时,会向zookeeper注册自己持有的topic和partitions信息,仍然是一个临时znode.

Consumer and Consumer group: 每个consumer客户端被创建时,会向zookeeper注册自己的信息;此作用主要是为了"负载均衡".一个group中的多个consumer可以交错的消费一个topic的所有partitions;简而言之,保证此topic的所有partitions都能被此group所消费,且消费时为了性能考虑,让partition相对均衡的分散到每个consumer上.

Consumer id Registry: 每个consumer都有一个唯一的ID(host:uuid,可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.

Consumer offset Tracking: 用来跟踪每个consumer目前所消费的partition中最大的offset.此znode为持久节点,可以看出offset跟group_id有关,以表明当group中一个消费者失效,其他consumer可以继续消费.

Partition Owner registry: 用来标记partition正在被哪个consumer消费.临时znode。此节点表达了"一个partition"只能被group下一个consumer消费,同时当group下某个consumer失效,那么将会触发负载均衡(即:让partitions在多个consumer间均衡消费,接管那些"游离"的partitions)

当consumer启动时,所触发的操作:

A) 首先进行"Consumer id Registry";

B) 然后在"Consumer id Registry"节点下注册一个watch用来监听当前group中其他consumer的"leave"和"join";只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions).

C) 在"Broker id registry"节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance.

 

总结:

1) Producer端使用zookeeper用来"发现"broker列表,以及和Topic下每个partition leader建立socket连接并发送消息.

2) Broker端使用zookeeper用来注册broker信息,已经监测partition leader存活性.

3) Consumer端使用zookeeper用来注册consumer信息,其中包括consumer消费的partition列表等,同时也用来发现broker列表,并和partition leader建立socket连接,并获取消息。

 

9.Leader的选择

Kafka的核心是日志文件，日志文件在集群中的同步是分布式数据系统最基础的要素。

如果leaders永远不会down的话我们就不需要followers了！一旦leader down掉了，需要在followers中选择一个新的leader.但是followers本身有可能延时太久或者crash，所以必须选择高质量的follower作为leader.必须保证，一旦一个消息被提交了，但是leader down掉了，新选出的leader必须可以提供这条消息。大部分的分布式系统采用了多数投票法则选择新的leader,对于多数投票法则，就是根据所有副本节点的状况动态的选择最适合的作为leader.Kafka并不是使用这种方法。

Kafka动态维护了一个同步状态的副本的集合（a set of in-sync replicas），简称ISR，在这个集合中的节点都是和leader保持高度一致的，任何一条消息必须被这个集合中的每个节点读取并追加到日志中了，才回通知外部这个消息已经被提交了。因此这个集合中的任何一个节点随时都可以被选为leader.ISR在ZooKeeper中维护。ISR中有f+1个节点，就可以允许在f个节点down掉的情况下不会丢失消息并正常提供服。ISR的成员是动态的，如果一个节点被淘汰了，当它重新达到“同步中”的状态时，他可以重新加入ISR.这种leader的选择方式是非常快速的，适合kafka的应用场景。

一个邪恶的想法：如果所有节点都down掉了怎么办？Kafka对于数据不会丢失的保证，是基于至少一个节点是存活的，一旦所有节点都down了，这个就不能保证了。

实际应用中，当所有的副本都down掉时，必须及时作出反应。可以有以下两种选择:

\1. 等待ISR中的任何一个节点恢复并担任leader。

\2. 选择所有节点中（不只是ISR）第一个恢复的节点作为leader.

这是一个在可用性和连续性之间的权衡。如果等待ISR中的节点恢复，一旦ISR中的节点起不起来或者数据都是了，那集群就永远恢复不了了。如果等待ISR意外的节点恢复，这个节点的数据就会被作为线上数据，有可能和真实的数据有所出入，因为有些数据它可能还没同步到。Kafka目前选择了第二种策略，在未来的版本中将使这个策略的选择可配置，可以根据场景灵活的选择。

这种窘境不只Kafka会遇到，几乎所有的分布式数据系统都会遇到。

 

10.副本管理

以上仅仅以一个topic一个分区为例子进行了讨论，但实际上一个Kafka将会管理成千上万的topic分区.Kafka尽量的使所有分区均匀的分布到集群所有的节点上而不是集中在某些节点上，另外主从关系也尽量均衡这样每个几点都会担任一定比例的分区的leader.

优化leader的选择过程也是很重要的，它决定了系统发生故障时的空窗期有多久。Kafka选择一个节点作为“controller”,当发现有节点down掉的时候它负责在游泳分区的所有节点中选择新的leader,这使得Kafka可以批量的高效的管理所有分区节点的主从关系。如果controller down掉了，活着的节点中的一个会备切换为新的controller.

 

11.Leader与副本同步

对于某个分区来说，保存正分区的"broker"为该分区的"leader"，保存备份分区的"broker"为该分区的"follower"。备份分区会完全复制正分区的消息，包括消息的编号等附加属性值。为了保持正分区和备份分区的内容一致，Kafka采取的方案是在保存备份分区的"broker"上开启一个消费者进程进行消费，从而使得正分区的内容与备份分区的内容保持一致。一般情况下，一个分区有一个“正分区”和零到多个“备份分区”。可以配置“正分区+备份分区”的总数量，关于这个配置，不同主题可以有不同的配置值。注意，生产者，消费者只与保存正分区的"leader"进行通信。

 

Kafka允许topic的分区拥有若干副本，这个数量是可以配置的，你可以为每个topic配置副本的数量。Kafka会自动在每个副本上备份数据，所以当一个节点down掉时数据依然是可用的。

Kafka的副本功能不是必须的，你可以配置只有一个副本，这样其实就相当于只有一份数据。

创建副本的单位是topic的分区，每个分区都有一个leader和零或多个followers.所有的读写操作都由leader处理，一般分区的数量都比broker的数量多的多，各分区的leader均匀的分布在brokers中。所有的followers都复制leader的日志，日志中的消息和顺序都和leader中的一致。followers向普通的consumer那样从leader那里拉取消息并保存在自己的日志文件中。

许多分布式的消息系统自动的处理失败的请求，它们对一个节点是否着（alive）”有着清晰的定义。Kafka判断一个节点是否活着有两个条件：

\1. 节点必须可以维护和ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接。

\2. 如果节点是个follower,他必须能及时的同步leader的写操作，延时不能太久。

符合以上条件的节点准确的说应该是“同步中的（in sync）”，而不是模糊的说是“活着的”或是“失败的”。Leader会追踪所有“同步中”的节点，一旦一个down掉了，或是卡住了，或是延时太久，leader就会把它移除。至于延时多久算是“太久”，是由参数replica.lag.max.messages决定的，怎样算是卡住了，怎是由参数replica.lag.time.max.ms决定的。

只有当消息被所有的副本加入到日志中时，才算是“committed”，只有committed的消息才会发送给consumer，这样就不用担心一旦leader down掉了消息会丢失。Producer也可以选择是否等待消息被提交的通知，这个是由参数acks决定的。

Kafka保证只要有一个“同步中”的节点，“committed”的消息就不会丢失。

 

 

- **2.3  kafka拓扑结构**

  ![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107211822608)

​    一个典型的Kafka集群中包含若干Producer（可以是web前端FET，或者是服务器日志等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干ConsumerGroup，以及一个Zookeeper集群。Kafka通过Zookeeper管理Kafka集群配置：选举Kafka broker的leader，以及在Consumer Group发生变化时进行rebalance，因为consumer消费kafka topic的partition的offsite信息是存在Zookeeper的。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。

 

分析过程分为以下4个步骤：

- topic中partition存储分布
- partiton中文件存储方式 (partition在linux服务器上就是一个目录（文件夹）)
- partiton中segment文件存储结构
- 在partition中如何通过offset查找message

通过上述4过程详细分析，我们就可以清楚认识到kafka文件存储机制的奥秘。

 

2.3 topic中partition存储分布

假设实验环境中Kafka集群只有一个broker，xxx/message-folder为数据文件存储根目录，在Kafka broker中server.properties文件配置(参数log.dirs=xxx/message-folder)，例如创建2个topic名 称分别为report_push、launch_info, partitions数量都为partitions=4

存储路径和目录规则为：

xxx/message-folder

 |--report_push-0
 |--report_push-1
 |--report_push-2
 |--report_push-3
 |--launch_info-0
 |--launch_info-1
 |--launch_info-2
 |--launch_info-3

 

在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为 topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。

消息发送时都被发送到一个topic，其本质就是一个目录，而topic由是由一些Partition组成,其组织结构如下图所示：

 

我们可以看到，Partition是一个Queue的结构，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition上，其中的每一个消息都被赋予了一个唯一的offset值。

 

Kafka集群会保存所有的消息，不管消息有没有被消费；**我们可以设定消息的过期时间，只有过期的数据才会被自动清除以释放磁盘空间。**比如我们设置消息过期时间为2天，那么这2天内的所有消息都会被保存到集群中，数据只有超过了两天才会被清除。

 

Kafka只维护在Partition中的offset值，因为这个offsite标识着这个partition的message消费到哪条了。Consumer每消费一个消息，offset就会加1。其实消息的状态完全是由Consumer控制的，Consumer可以跟踪和重设这个offset值，这样的话Consumer就可以读取任意位置的消息。

 

把消息日志以Partition的形式存放有多重考虑，第一，方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；第二就是可以提高并发，因为可以以Partition为单位读写了。

 

通过上面介绍的我们可以知道，kafka中的数据是持久化的并且能够容错的。Kafka允许用户为每个topic设置副本数量，副本数量决定了有几个broker来存放写入的数据。如果你的副本数量设置为3，那么一份数据就会被存放在3台不同的机器上，那么就允许有2个机器失败。一般推荐副本数量至少为2，这样就可以保证增减、重启机器时不会影响到数据消费。如果对数据持久化有更高的要求，可以把副本数量设置为3或者更多。

 

Kafka中的topic是以partition的形式存放的，每一个topic都可以设置它的partition数量，Partition的数量决定了组成topic的message的数量。Producer在生产数据时，会按照一定规则（这个规则是可以自定义的）把消息发布到topic的各个partition中。上面将的副本都是以partition为单位的，不过只有一个partition的副本会被选举成leader作为读写用。

 

关于如何设置partition值需要考虑的因素。**一个partition只能被一个消费者消费（一个消费者可以同时消费多个partition）**，因此，如果设置的partition的数量小于consumer的数量，就会有消费者消费不到数据。所以，推荐partition的数量一定要大于同时运行的consumer的数量。另外一方面，建议partition的数量大于集群broker的数量，这样leader partition就可以均匀的分布在各个broker中，最终使得集群负载均衡。在Cloudera,每个topic都有上百个partition。需要注意的是，kafka需要为每个partition分配一些内存来缓存消息数据，如果partition数量越大，就要为kafka分配更大的heap space。

2.4 partiton中文件存储方式

 

- 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。
- 每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。

这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107212036116)

2.5 partiton中segment文件存储结构

producer发message到某个topic，message会被均匀的分布到多个partition上（随机或根据用户指定的回调函数进行分布），kafka broker收到message往对应partition的最后一个segment上添加该消息，当某个segment上的消息条数达到配置值或消息发布时间超过阈值时，segment上的消息会被flush到磁盘，只有flush到磁盘上的消息consumer才能消费，segment达到一定的大小后将不会再往该segment写数据，broker会创建新的segment。

 

每个part在内存中对应一个index，记录每个segment中的第一条消息偏移。

- segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀".index"和“.log”分别表示为segment索引文件、数据文件.
- segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个全局partion的最大offset(偏移message数)。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。

 

每个segment中存储很多条消息，消息id由其逻辑位置决定，即从消息id可直接定位到消息的存储位置，避免id到位置的额外映射。

下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB,并启动producer向Kafka broker写入大量数据,如下图2所示segment文件列表形象说明了上述2个规则：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107212205785)

以上述图2中一对segment file文件为例，说明segment中index<—->data file对应关系物理结构如下：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107212224538)

上述图3中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。其中以索引文件中 元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移 地址为497。

从上述图3了解到segment data file由许多message组成，下面详细说明message物理结构如下：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107212325100)

### 参数说明：

| 关键字              | 解释说明                                                     |
| :------------------ | :----------------------------------------------------------- |
| 8 byte offset       | 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即**offset表示partiion的第多少message** |
| 4 byte message size | message大小                                                  |
| 4 byte CRC32        | 用crc32校验message                                           |
| 1 byte “magic"      | 表示本次发布Kafka服务程序协议版本号                          |
| 1 byte “attributes" | 表示为独立版本、或标识压缩类型、或编码类型。                 |
| 4 byte key length   | 表示key的长度,当key为-1时，K byte key字段不填                |
| K byte key          | 可选                                                         |
| value bytes payload | 表示实际消息数据。                                           |

 

2.6 在partition中如何通过offset查找message

例如读取offset=368776的message，需要通过下面2个步骤查找。

- 第一步查找segment file

  上述图2为例，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件 00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset **二分查找**文件列表，就可以快速定位到具体文件。

  当offset=368776时定位到00000000000000368769.index|log

- 第二步通过segment file查找message通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和 00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到 offset=368776为止。

segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它 比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。

 

kafka会记录offset到zk中。但是，zk client api对zk的频繁写入是一个低效的操作。0.8.2 kafka引入了native offset storage，将offset管理从zk移出，并且可以做到水平扩展。其原理就是利用了kafka的compacted topic，offset以consumer group,topic与partion的组合作为key直接提交到compacted topic中。同时Kafka又在内存中维护了的三元组来维护最新的offset信息，consumer来取最新offset信息的时候直接内存里拿即可。当然，kafka允许你快速的checkpoint最新的offset信息到磁盘上。

 

3.Partition Replication原则

Kafka高效文件存储设计特点

- Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。
- 通过索引信息可以快速定位message和确定response的最大大小。
- 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。
- 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。

 

 

\1. Kafka集群partition replication默认自动分配分析

下面以一个Kafka集群中4个Broker举例，创建1个topic包含4个Partition，2 Replication；数据Producer流动如图所示：

(1)

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107212450867)

 

 

(2)当集群中新增2节点，Partition增加到6个时分布情况如下：

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107212535915)

 

副本分配逻辑规则如下：

- 在Kafka集群中，每个Broker都有均等分配Partition的Leader机会。
- 上述图Broker Partition中，箭头指向为副本，以Partition-0为例:broker1中parition-0为Leader，Broker2中Partition-0为副本。
- 上述图种每个Broker(按照BrokerId有序)依次分配主Partition,下一个Broker为副本，如此循环迭代分配，多副本都遵循此规则。

 

副本分配算法如下：

- 将所有N Broker和待分配的i个Partition排序.
- 将第i个Partition分配到第(i mod n)个Broker上.
- 将第i个Partition的第j个副本分配到第((i + j) mod n)个Broker上.

 

4.Kafka Broker一些特性

***\*4.1 无状态的Kafka Broker :\****

\1. Broker没有副本机制，一旦broker宕机，该broker的消息将都不可用。

\2. Broker不保存订阅者的状态，由订阅者自己保存。

\3. 无状态导致消息的删除成为难题（可能删除的消息正在被订阅），kafka采用基于时间的SLA(服务水平保证)，消息保存一定时间（通常为7天）后会被删除。

\4. 消息订阅者可以rewind back到任意位置重新进行消费，当订阅者故障时，可以选择最小的offset进行重新读取消费消息。

 

***\*4.2 message的交付与生命周期 ：\****

\1. 不是严格的JMS， 因此 kafka对消息的重复、丢失、错误以及顺序型没有严格的要求。**（这是与AMQ最大的区别）**

\2. kafka提供at-least-once delivery,即当consumer宕机后，有些消息可能会被重复delivery。

\3. 因每个partition只会被consumer group内的一个consumer消费，故kafka保证每个partition内的消息会被顺序的订阅。

\4. Kafka为每条消息为每条消息计算CRC校验，用于错误检测，crc校验不通过的消息会直接被丢弃掉。

 

***\*4.3 压缩\****

 

Kafka支持以集合（batch）为单位发送消息，在此基础上，Kafka还支持对消息集合进行压缩，Producer端可以通过GZIP或Snappy格式对消息集合进行压缩。Producer端进行压缩之后，在Consumer端需进行解压。压缩的好处就是减少传输的数据量，减轻对网络传输的压力，在对大数据处理上，瓶颈往往体现在网络上而不是CPU。

 

那么如何区分消息是压缩的还是未压缩的呢，Kafka在消息头部添加了一个描述压缩属性字节，这个字节的后两位表示消息的压缩采用的编码，如果后两位为0，则表示消息未被压缩。

 

***\*4.4 消息可靠性\****

 

在消息系统中，保证消息在生产和消费过程中的可靠性是十分重要的，在实际消息传递过程中，可能会出现如下三中情况：

 

\- 一个消息发送失败

 

\- 一个消息被发送多次

 

\- 最理想的情况：exactly-once ,一个消息发送成功且仅发送了一次

 

有许多系统声称它们实现了exactly-once，但是它们其实忽略了生产者或消费者在生产和消费过程中有可能失败的情况。比如虽然一个Producer成功发送一个消息，但是消息在发送途中丢失，或者成功发送到broker，也被consumer成功取走，但是这个consumer在处理取过来的消息时失败了。

 

从Producer端看：Kafka是这么处理的，当一个消息被发送后，Producer会等待broker成功接收到消息的反馈（可通过参数控制等待时间），如果消息在途中丢失或是其中一个broker挂掉，Producer会重新发送（我们知道Kafka有备份机制，可以通过参数控制是否等待所有备份节点都收到消息）。

 

从Consumer端看：前面讲到过partition，broker端记录了partition中的一个offset值，这个值指向Consumer下一个即将消费message。当Consumer收到了消息，但却在处理过程中挂掉，此时Consumer可以通过这个offset值重新找到上一个消息再进行处理。Consumer还有权限控制这个offset值，对持久化到broker端的消息做任意处理。

 

***\*4.5 备份机制\****

 

备份机制是Kafka0.8版本的新特性，备份机制的出现大大提高了Kafka集群的可靠性、稳定性。有了备份机制后，Kafka允许集群中的节点挂掉后而不影响整个集群工作。一个备份数量为n的集群允许n-1个节点失败。在所有备份节点中，有一个节点作为lead节点，这个节点保存了其它备份节点列表，并维持各个备份间的状体同步。下面这幅图解释了Kafka的备份机制:

 

 

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107212704151)

***\*4.6 Kafka高效性相关设计\****

 

***\*4.6.1 消息的持久化\****

Kafka高度依赖文件系统来存储和缓存消息(AMQ的nessage是持久化到mysql数据库中的)，因为一般的人认为磁盘是缓慢的，这导致人们对持久化结构具有竞争性持怀疑态度。其实，磁盘的快或者慢，这决定于我们如何使用磁盘。 因为磁盘线性写的速度远远大于随机写。线性读写在大多数应用场景下是可以预测的。

***\*4.6.2 常数时间性能保证\****

每个Topic的Partition的是一个大文件夹，里面有无数个小文件夹segment，但partition是一个队列，队列中的元素是segment,消费的时候先从第0个segment开始消费，新来message存在最后一个消息队列中。对于segment也是对队列，队列元素是message,有对应的offsite标识是哪个message。消费的时候先从这个segment的第一个message开始消费，新来的message存在segment的最后。

 

消息系统的 持久化队列可以构建在对一个文件的读和追加上，就像一般情况下的日志解决方案。它有一个优点，所有的操作都是常数时间，并且读写之间不会相互阻塞。这种设计具有极大的性能优势：最终系统性能和数据大小完全无关，服务器可以充分利用廉价的硬盘来提供高效的消息服务。

 

事实上还有一点，磁盘空间的无限增大而不影响性能这点，意味着我们可以提供一般消息系统无法提供的特性。比如说，消息被消费后不是立马被删除，我们可以将这些消息保留一段相对比较长的时间（比如一个星期）。

 

5.Kafka 生产者-消费者

   消息系统通常都会由生产者，消费者，Broker三大部分组成，生产者会将消息写入到Broker，消费者会从Broker中读取出消息，不同的MQ实现的Broker实现会有所不同，不过Broker的本质都是要负责将消息落地到服务端的存储系统中。具体步骤如下：

1. 生产者客户端应用程序产生消息：
   1. 客户端连接对象将消息包装到请求中发送到服务端
   2. 服务端的入口也有一个连接对象负责接收请求，并将消息以文件的形式存储起来
   3. 服务端返回响应结果给生产者客户端
2. 消费者客户端应用程序消费消息：
   1. 客户端连接对象将消费信息也包装到请求中发送给服务端
   2. 服务端从文件存储系统中取出消息
   3. 服务端返回响应结果给消费者客户端
   4. 客户端将响应结果还原成消息并开始处理消息

 

​                                       图4-1 客户端和服务端交互

 

**5.1  Producers**

 

Producers直接发送消息到broker上的leader partition，不需要经过任何中介或其他路由转发。为了实现这个特性，kafka集群中的每个broker都可以响应producer的请求，并返回topic的一些元信息，这些元信息包括哪些机器是存活的，topic的leader partition都在哪，现阶段哪些leader partition是可以直接被访问的。

 

**Producer客户端自己控制着消息被推送到哪些partition。**实现的方式可以是随机分配、实现一类随机负载均衡算法，或者指定一些分区算法。Kafka提供了接口供用户实现自定义的partition，用户可以为每个消息指定一个partitionKey，通过这个key来实现一些hash分区算法。比如，把userid作为partitionkey的话，相同userid的消息将会被推送到同一个partition。

 

以Batch的方式推送数据可以极大的提高处理效率，kafka Producer 可以将消息在内存中累计到一定数量后作为一个batch发送请求。Batch的数量大小可以通过Producer的参数控制，参数值可以设置为累计的消息的数量（如500条）、累计的时间间隔（如100ms）或者累计的数据大小(64KB)。通过增加batch的大小，可以减少网络请求和磁盘IO的次数，当然具体参数设置需要在效率和时效性方面做一个权衡。

 

Producers可以异步的并行的向kafka发送消息，但是通常producer在发送完消息之后会得到一个future响应，返回的是offset值或者发送过程中遇到的错误。这其中有个非常重要的参数“acks”,这个参数决定了producer要求leader partition 收到确认的副本个数，如果acks设置数量为0，表示producer不会等待broker的响应，所以，producer无法知道消息是否发送成功，这样有可能会导致数据丢失，但同时，acks值为0会得到最大的系统吞吐量。

 

若acks设置为1，表示producer会在leader partition收到消息时得到broker的一个确认，这样会有更好的可靠性，因为客户端会等待直到broker确认收到消息。若设置为-1，producer会在所有备份的partition收到消息时得到broker的确认，这个设置可以得到最高的可靠性保证。

 

Kafka 消息有一个定长的header和变长的字节数组组成。因为kafka消息支持字节数组，也就使得kafka可以支持任何用户自定义的序列号格式或者其它已有的格式如Apache Avro、protobuf等。Kafka没有限定单个消息的大小，但我们推荐消息大小不要超过1MB,通常一般消息大小都在1~10kB之前。

 

发布消息时，kafka client先构造一条消息，将消息加入到消息集set中（kafka支持批量发布，可以往消息集合中添加多条消息，一次行发布），send消息时，producer client需指定消息所属的topic。

 

**5.2  Consumers**

Kafka提供了两套consumer api，分为high-level api和sample-api。Sample-api 是一个底层的API，它维持了一个和单一broker的连接，并且这个API是完全无状态的，每次请求都需要指定offset值，因此，这套API也是最灵活的。

 

**在kafka中，当前读到哪条消息的offset值是由consumer来维护的**，因此，consumer可以自己决定如何读取kafka中的数据。**比如，consumer可以通过重设offset值来重新消费已消费过的数据。不管有没有被消费，kafka会保存数据一段时间，这个时间周期是可配置的，只有到了过期时间，kafka才会删除这些数据。（这一点与AMQ不一样，AMQ的message一般来说都是持久化到mysql中的，消费完的message会被delete掉）**

 

High-level API封装了对集群中一系列broker的访问，可以透明的消费一个topic。它自己维持了已消费消息的状态，即每次消费的都是下一个消息。

 

High-level API还支持以组的形式消费topic，如果consumers有同一个组名，那么kafka就相当于一个队列消息服务，而各个consumer均衡的消费相应partition中的数据。若consumers有不同的组名，那么此时kafka就相当与一个广播服务，会把topic中的所有消息广播到每个consumer。

 

High level api和Low level api是针对consumer而言的，和producer无关。

 

High level api是consumer读的partition的offsite是存在zookeeper上。High level api 会启动另外一个线程去每隔一段时间，offsite自动同步到zookeeper上。换句话说，如果使用了High level api， 每个message只能被读一次，一旦读了这条message之后，无论我consumer的处理是否ok。High level api的另外一个线程会自动的把offiste+1同步到zookeeper上。如果consumer读取数据出了问题，offsite也会在zookeeper上同步。因此，如果consumer处理失败了，会继续执行下一条。这往往是不对的行为。因此，Best Practice是一旦consumer处理失败，直接让整个conusmer group抛Exception终止，但是最后读的这一条数据是丢失了，因为在zookeeper里面的offsite已经+1了。等再次启动conusmer group的时候，已经从下一条开始读取处理了。

 

Low level api 是consumer读的partition的offsite在consumer自己的程序中维护。不会同步到zookeeper上。但是为了kafka manager能够方便的监控，一般也会手动的同步到zookeeper上。这样的好处是一旦读取某个message的consumer失败了，这条message的offsite我们自己维护，我们不会+1。下次再启动的时候，还会从这个offsite开始读。这样可以做到exactly once对于数据的准确性有保证。

 

 

对于Consumer group：

\1. 允许consumer group（包含多个consumer，如一个集群同时消费）对一个topic进行消费，不同的consumer group之间独立消费。

\2. 为了对减小一个consumer group中不同consumer之间的分布式协调开销，指定partition为最小的并行消费单位，即一个group内的consumer只能消费不同的partition。

![img](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/20170107212800371)

 

 

Consumer与Partition的关系：

\- 如果consumer比partition多，是浪费，因为kafka的设计是在一个partition上是不允许并发的，所以consumer数不要大于partition数

\- 如果consumer比partition少，一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀

\- 如果consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同

\- 增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化

\- High-level接口中获取不到数据的时候是会block的

 

负载低的情况下可以每个线程消费多个partition。但负载高的情况下，Consumer 线程数最好和Partition数量保持一致。如果还是消费不过来，应该再开 Consumer 进程，进程内线程数同样和分区数一致。

 

消费消息时，kafka client需指定topic以及partition number（每个partition对应一个逻辑日志流，如topic代表某个产品线，partition代表产品线的日志按天切分的结果），consumer client订阅后，就可迭代读取消息，如果没有消息，consumer client会阻塞直到有新的消息发布。consumer可以累积确认接收到的消息，当其确认了某个offset的消息，意味着之前的消息也都已成功接收到，此时broker会更新zookeeper上地offset registry。

 

***\*5.3\****  高效的数据传输

\1. 发布者每次可发布多条消息（将消息加到一个消息集合中发布）， consumer每次迭代消费一条消息。

\2. 不创建单独的cache，使用系统的page cache。发布者顺序发布，订阅者通常比发布者滞后一点点，直接使用[Linux](http://lib.csdn.net/base/linux)的page cache效果也比较后，同时减少了cache管理及垃圾收集的开销。

\3. 使用sendfile优化网络传输，减少一次内存拷贝。

 

6.Kafka 与 Zookeeper

 

6.1 Zookeeper 协调控制

\1. 管理broker与consumer的动态加入与离开。(Producer不需要管理，随便一台计算机都可以作为Producer向Kakfa Broker发消息)

\2. 触发负载均衡，当broker或consumer加入或离开时会触发负载均衡算法，使得一

  个consumer group内的多个consumer的消费负载平衡。（因为一个comsumer消费一个或多个partition，一个partition只能被一个consumer消费）

\3. 维护消费关系及每个partition的消费信息。

 

6.2 Zookeeper上的细节：

\1. 每个broker启动后会在zookeeper上注册一个临时的broker registry，包含broker的ip地址和端口号，所存储的topics和partitions信息。

\2. 每个consumer启动后会在zookeeper上注册一个临时的consumer registry：包含consumer所属的consumer group以及订阅的topics。

\3. 每个consumer group关联一个临时的owner registry和一个持久的offset registry。对于被订阅的每个partition包含一个owner registry，内容为订阅这个partition的consumer id；同时包含一个offset registry，内容为上一次订阅的offset。





# 5.SpringCloud Gateway 

SpringCloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。

而为了提升网关的性能，SpringCloud Gateway是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty。

Spring Cloud Gateway 的目标，不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。

提前声明：**Spring Cloud Gateway 底层使用了高性能的通信框架Netty**。

## **1.2** SpringCloud Gateway 特征

SpringCloud官方，对SpringCloud Gateway 特征介绍如下：

（1）基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0

（2）集成 Hystrix 断路器

（3）集成 Spring Cloud DiscoveryClient

（4）Predicates 和 Filters 作用于特定路由，易于编写的 Predicates 和 Filters

（5）具备一些网关的高级功能：动态路由、限流、路径重写

从以上的特征来说，和Zuul的特征差别不大。SpringCloud Gateway和Zuul主要的区别，还是在底层的通信框架上。

简单说明一下上文中的三个术语：

**（**1**）**Filter**（过滤器）**：

和Zuul的过滤器在概念上类似，可以使用它拦截和修改请求，并且对上游的响应，进行二次处理。过滤器为org.springframework.cloud.gateway.filter.GatewayFilter类的实例。

（2）**Route**（路由）：

网关配置的基本组成模块，和Zuul的路由配置模块类似。一个**Route模块**由一个 ID，一个目标 URI，一组断言和一组过滤器定义。如果断言为真，则路由匹配，目标URI会被访问。

**（**3**）**Predicate**（断言）**：

这是一个 Java 8 的 Predicate，可以使用它来匹配来自 HTTP 请求的任何内容，例如 headers 或参数。**断言的**输入类型是一个 ServerWebExchange。

## **1.3** SpringCloud Gateway和架构

Spring在2017年下半年迎来了Webflux，Webflux的出现填补了Spring在响应式编程上的空白，Webflux的响应式编程不仅仅是编程风格的改变，而且对于一系列的著名框架，都提供了响应式访问的开发包，比如Netty、Redis等等。

SpringCloud Gateway 使用的Webflux中的reactor-netty响应式编程组件，底层使用了Netty通讯框架。
[![在这里插入图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/19816137-8758f092be21e6f7.gif)](https://upload-images.jianshu.io/upload_images/19816137-8758f092be21e6f7.gif?imageMogr2/auto-orient/strip)

### **1.3.1** SpringCloud Zuul的IO模型

Springcloud中所集成的Zuul版本，采用的是Tomcat容器，使用的是传统的Servlet IO处理模型。

大家知道，servlet由servlet container进行生命周期管理。container启动时构造servlet对象并调用servlet init()进行初始化；container关闭时调用servlet destory()销毁servlet；container运行时接受请求，并为每个请求分配一个线程（一般从线程池中获取空闲线程）然后调用service()。

弊端：servlet是一个简单的网络IO模型，当请求进入servlet container时，servlet container就会为其绑定一个线程，在并发不高的场景下这种模型是适用的，但是一旦并发上升，线程数量就会上涨，而线程资源代价是昂贵的（上线文切换，内存消耗大）严重影响请求的处理时间。在一些简单的业务场景下，不希望为每个request分配一个线程，只需要1个或几个线程就能应对极大并发的请求，这种业务场景下servlet模型没有优势。
[![在这里插入图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/19816137-bb466f6b0135bb71)](https://upload-images.jianshu.io/upload_images/19816137-bb466f6b0135bb71?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

所以Springcloud Zuul 是基于servlet之上的一个阻塞式处理模型，即spring实现了处理所有request请求的一个servlet（DispatcherServlet），并由该servlet阻塞式处理处理。所以Springcloud Zuul无法摆脱servlet模型的弊端。虽然Zuul 2.0开始，使用了Netty，并且已经有了大规模Zuul 2.0集群部署的成熟案例，但是，Springcloud官方已经没有集成改版本的计划了。

### **1.3.2** Webflux模型

Webflux模式替换了旧的Servlet线程模型。用少量的线程处理request和response io操作，这些线程称为Loop线程，而业务交给响应式编程框架处理，响应式编程是非常灵活的，用户可以将业务中阻塞的操作提交到响应式框架的work线程中执行，而不阻塞的操作依然可以在Loop线程中进行处理，大大提高了Loop线程的利用率。官方结构图：

[![在这里插入图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/19816137-dad0e43fc31f4536)](https://upload-images.jianshu.io/upload_images/19816137-dad0e43fc31f4536?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

Webflux虽然可以兼容多个底层的通信框架，但是一般情况下，底层使用的还是Netty，毕竟，Netty是目前业界认可的最高性能的通信框架。而Webflux的Loop线程，正好就是著名的Reactor 模式IO处理模型的Reactor线程，如果使用的是高性能的通信框架Netty，这就是Netty的EventLoop线程。

关于Reactor线程模型，和Netty通信框架的知识，是Java程序员的重要、必备的内功，个中的原理，具体请参见尼恩编著的《Netty、Zookeeper、Redis高并发实战》一书，这里不做过多的赘述。

### **1.3.3** Spring Cloud Gateway的处理流程

客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（“pre”）或之后（“post”）执行业务逻辑。
[![在这里插入图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/19816137-eeedbd49be096c05)](https://upload-images.jianshu.io/upload_images/19816137-eeedbd49be096c05?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

## **1.4** Spring Cloud Gateway路由配置方式

### **1.4.1** 基础URI一种路由配置方式

如果请求的目标地址，是单个的URI资源路径，配置文件示例如下：



```
server:
  port: 8080
spring:
  application:
    name: api-gateway
  cloud:
    gateway:
      routes:
        -id: url-proxy-1
          uri: https://blog.csdn.net
          predicates:
            -Path=/csdn
```

各字段含义如下：

id：我们自定义的路由 ID，保持唯一

uri：目标服务地址

predicates：路由条件，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。

上面这段配置的意思是，配置了一个 id 为 url-proxy-1的URI代理规则，路由的规则为：当访问地址http://localhost:8080/csdn/1.jsp时，会路由到上游地址https://blog.csdn.net/1.jsp。

### **1.4.2** 基于代码的路由配置方式

转发功能同样可以通过代码来实现，我们可以在启动类 GateWayApplication 中添加方法 customRouteLocator() 来定制转发规则。



```
package com.springcloud.gateway;
 
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
 
@SpringBootApplication
public class GatewayApplication {
 
    public static void main(String[] args) {
        SpringApplication.run(GatewayApplication.class, args);
    }
 
    @Bean
    public RouteLocator customRouteLocator(RouteLocatorBuilder builder) {
        return builder.routes()
                .route("path_route", r -> r.path("/csdn")
                        .uri("https://blog.csdn.net"))
                .build();
    }
 
}
```

我们在yaml配置文件中注销掉相关路由的配置，重启服务，访问链接：http://localhost:8080/ csdn， 可以看到和上面一样的页面，证明我们测试成功。

上面两个示例中 uri 都是指向了我的CSDN博客，在实际项目使用中可以将 uri 指向对外提供服务的项目地址，统一对外输出接口。

### **1.4.3** 和注册中心相结合的路由配置方式

在uri的schema协议部分为自定义的lb:类型，表示从微服务注册中心（如Eureka）订阅服务，并且进行服务的路由。

一个典型的示例如下：



```
server:
  port: 8084
spring:
  cloud:
    gateway:
      routes:
      -id: seckill-provider-route
        uri: lb://seckill-provider
        predicates:
        - Path=/seckill-provider/**

      -id: message-provider-route
        uri: lb://message-provider
        predicates:
        -Path=/message-provider/**

application:
  name: cloud-gateway

eureka:
  instance:
    prefer-ip-address: true
  client:
    service-url:
      defaultZone: http://localhost:8888/eureka/
```

注册中心相结合的路由配置方式，与单个URI的路由配置，区别其实很小，仅仅在于URI的schema协议不同。单个URI的地址的schema协议，一般为http或者https协议。

## **1.5** 详解：SpringCloud Gateway 匹配规则

Spring Cloud Gateway 的功能很强大，我们仅仅通过 Predicates 的设计就可以看出来，前面我们只是使用了 predicates 进行了简单的条件匹配，其实 Spring Cloud Gataway 帮我们内置了很多 Predicates 功能。

Spring Cloud Gateway 是通过 Spring WebFlux 的 HandlerMapping 做为底层支持来匹配到转发路由，Spring Cloud Gateway 内置了很多 Predicates 工厂，这些 Predicates 工厂通过不同的 HTTP 请求参数来匹配，多个 Predicates 工厂可以组合使用。

### **1.5.1** Predicate 断言条件介绍

Predicate 来源于 Java 8，是 Java 8 中引入的一个函数，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。可以用于接口请求参数校验、判断新老数据是否有变化需要进行更新操作。

在 Spring Cloud Gateway 中 Spring 利用 Predicate 的特性实现了各种路由匹配规则，有通过 Header、请求参数等不同的条件来进行作为条件匹配到对应的路由。网上有一张图总结了 Spring Cloud 内置的几种 Predicate 的实现。
[![在这里插入图片描述](21%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95.assets/19816137-bb046dbf19bee1b4.gif)](https://upload-images.jianshu.io/upload_images/19816137-bb046dbf19bee1b4.gif?imageMogr2/auto-orient/strip)
[
说白了 Predicate 就是为了实现一组匹配规则，方便让请求过来找到对应的 Route 进行处理，接下来我们接下 Spring Cloud GateWay 内置几种 Predicate 的使用。

### **1.5.2** 通过请求参数匹配

Query Route Predicate 支持传入两个参数，一个是属性名一个为属性值，属性值可以是正则表达式。



```
server:

  port: 8080

spring:

  application:

    name: api-gateway

  cloud:

    gateway:

      routes:

        -id: gateway-service

          uri: https://www.baidu.com

          order: 0

          predicates:

            -Query=smile
```

这样配置，只要请求中包含 smile 属性的参数即可匹配路由。

使用 curl 测试，命令行输入:

curl localhost:8080?smile=x&id=2

经过测试发现只要请求汇总带有 smile 参数即会匹配路由，不带 smile 参数则不会匹配。

还可以将 Query 的值以键值对的方式进行配置，这样在请求过来时会对属性值和正则进行匹配，匹配上才会走路由。



```
server:

  port: 8080

spring:

  application:

    name: api-gateway

  cloud:

    gateway:

      routes:

        -id: gateway-service

          uri: https://www.baidu.com

          order: 0

          predicates:

            -Query=keep, pu.
```

这样只要当请求中包含 keep 属性并且参数值是以 pu 开头的长度为三位的字符串才会进行匹配和路由。

使用 curl 测试，命令行输入:

curl localhost:8080?keep=pub

测试可以返回页面代码，将 keep 的属性值改为 pubx 再次访问就会报 404,证明路由需要匹配正则表达式才会进行路由。